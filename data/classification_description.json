{"yeast": "**Author**:   \n**Source**: Unknown -   \n**Please cite**:", "KDDCup09_appetency": "**Author**:   \n**Source**: Unknown - Date unknown  \n**Please cite**:   \n\nDatasets from ACM KDD Cup (http://www.sigkdd.org/kddcup/index.php)\n\nKDD Cup 2009\nhttp://www.kddcup-orange.com\n\nConverted to ARFF format by TunedIT\nCustomer Relationship Management (CRM) is a key element of modern marketing strategies. The KDD Cup 2009 offers the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling).\nThe most practical way, in a CRM system, to build knowledge on customer is to produce scores. A score (the output of a model) is an evaluation for all instances of a target variable to explain (i.e. churn, appetency or up-selling). Tools which produce scores allow to project, on a given population, quantifiable information. The score is computed using input variables which describe instances. Scores are then used by the information system (IS), for example, to personalize the customer relationship. An industrial customer analysis platform able to build prediction models with a very large number of input variables has been developed by Orange Labs. This platform implements several processing methods for instances and variables selection, prediction and indexation based on an efficient model combined with variable selection regularization and model averaging method. The main characteristic of this platform is its ability to scale on very large datasets with hundreds of thousands of instances and thousands of variables. The rapid and robust detection of the variables that have most contributed to the output prediction can be a key factor in a marketing application.\nAppetency: In our context, the appetency is the propensity to buy a service or a product.\nThe training set contains 50,000 examples.\nThe first predictive 190 variables are numerical and the last 40 predictive variables are categorical.\nThe last target variable is binary {-1,1}.", "covertype": "**Author**: Jock A. Blackard, Dr. Denis J. Dean, Dr. Charles W. Anderson  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Covertype) - 1998  \n\nThis is the original version of the famous covertype dataset in ARFF format. \n\n**Covertype**  \nPredicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System &#40;RIS&#41; data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types). \n\nThis study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. \n\nSome background information for these four wilderness areas: Neota (area 2) probably has the highest mean elevational value of the 4 wilderness areas. Rawah (area 1) and Comanche Peak (area 3) would have a lower mean elevational value, while Cache la Poudre (area 4) would have the lowest mean elevational value. \n\nAs for primary major tree species in these areas, Neota would have spruce/fir (type 1), while Rawah and Comanche Peak would probably have lodgepole pine (type 2) as their primary species, followed by spruce/fir and aspen (type 5). Cache la Poudre would tend to have Ponderosa pine (type 3), Douglas-fir (type 6), and cottonwood/willow (type 4). \n\nThe Rawah and Comanche Peak areas would tend to be more typical of the overall dataset than either the Neota or Cache la Poudre, due to their assortment of tree species and range of predictive variable values (elevation, etc.) Cache la Poudre would probably be more unique than the others, due to its relatively low elevation range and species composition.\n\nAttribute Information:  \nGiven is the attribute name, attribute type, the measurement unit and a brief description. The forest cover type is the classification problem. The order of this listing corresponds to the order of numerals along the rows of the database. \n>\nName / Data Type / Measurement / Description  \nElevation / quantitative /meters / Elevation in meters  \nAspect / quantitative / azimuth / Aspect in degrees azimuth  \nSlope / quantitative / degrees / Slope in degrees  \nHorizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features  \nVertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features  \nHorizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway  \nHillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice  \nHillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer solstice  \nHillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice  \nHorizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points  \nWilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation  \nSoil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation  \nCover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation \n\n\nRelevant Papers:  \n- Blackard, Jock A. and Denis J. Dean. 2000. \"Comparative Accuracies of Artificial Neural Networks and Discriminant Analysis in Predicting Forest Cover Types from Cartographic Variables.\" Computers and Electronics in Agriculture 24(3):131-151. \n- Blackard, Jock A. and Denis J. Dean. 1998. \"Comparative Accuracies of Neural Networks and Discriminant Analysis in Predicting Forest Cover Types from Cartographic Variables.\" Second Southern Forestry GIS Conference. University of Georgia. Athens, GA. Pages 189-199. \n- Blackard, Jock A. 1998. \"Comparison of Neural Networks and Discriminant Analysis in Predicting Forest Cover Types.\" Ph.D. dissertation. Department of Forest Sciences. Colorado State University. Fort Collins, Colorado. 165 pages.", "amazon-commerce-reviews": "**Author**: Zhi Liu  \n**Source**: UCI\n**Please cite**:   \n\nDataset creator and donator: Zhi Liu, e-mail: liuzhi8673 '@' gmail.com, institution: National Engineering Research Center for E-Learning, Hubei Wuhan, China\n\nData Set Information:\n \ndataset are derived from the customers reviews in Amazon Commerce Website for authorship identification. Most previous studies conducted the identification experiments for two to ten authors. But in the online context, reviews to be identified usually have more potential authors, and normally classification algorithms are not adapted to large number of target classes. To examine the robustness of classification algorithms, we identified 50 of the most active users (represented by a unique ID and username) who frequently posted reviews in these newsgroups. The number of reviews we collected for each author is 30.\n\nAttribute Information:\n \nattribution includes authors' linguistic style such as usage of digit, punctuation, words and sentences' length and usage frequency of words and so on", "Australian": "**Author**: Confidential. Donated by Ross Quinlan  \n**Source**: [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html), [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Australian+Credit+Approval)) - 1987    \n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html) \n\n**Important note:** This dataset is derived from [credit-approval](https://www.openml.org/d/29), even though both datasets exist individually on UCI. In this version, missing values were filled in (not clear how) and a duplicate feature was removed.  \n\n**Australian Credit Approval**. This is the famous Australian Credit Approval dataset, originating from the StatLog project. It concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect the confidentiality of the data. \n\nThis dataset was retrieved 2014-11-14 from the UCI site and converted to the ARFF format.\n\n__Major changes w.r.t. version 3: dataset from UCI that matches description and data types__\n\n\n### Feature information\n\nThere are 6 numerical and 8 categorical attributes, all normalized to [-1,1]. The original formatting was as follows:  \n\nA1: 0,1 CATEGORICAL (formerly: a,b)  \nA2: continuous.  \nA3: continuous.  \nA4: 1,2,3 CATEGORICAL (formerly: p,g,gg)  \nA5: 1, 2,3,4,5, 6,7,8,9,10,11,12,13,14 CATEGORICAL (formerly: ff,d,i,k,j,aa,m,c,w, e, q, r,cc, x)  \nA6: 1, 2,3, 4,5,6,7,8,9 CATEGORICAL (formerly: ff,dd,j,bb,v,n,o,h,z)  \nA7: continuous.  \nA8: 1, 0 CATEGORICAL (formerly: t, f)  \nA9: 1, 0 CATEGORICAL (formerly: t, f)  \nA10: continuous.  \nA11: 1, 0 CATEGORICAL (formerly t, f)  \nA12: 1, 2, 3 CATEGORICAL (formerly: s, g, p)  \nA13: continuous.  \nA14: continuous.  \nA15: 1,2 class attribute (formerly: +,-)  \n\n### Relevant Papers\n\nRoss Quinlan. \"Simplifying decision trees\", Int J Man-Machine Studies 27, Dec 1987, pp. 221-234. \n\nRoss Quinlan. \"C4.5: Programs for Machine Learning\", Morgan Kaufmann, Oct 1992", "wilt": "**Author**: Brian Johnson     \n\n**Source**: [UCI] (https://archive.ics.uci.edu/ml/datasets/Wilt)  \n\n**Please cite**: Johnson, B., Tateishi, R., Hoan, N., 2013. A hybrid pansharpening approach and multiscale object-based image analysis for mapping diseased pine and oak trees. International Journal of Remote Sensing, 34 (20), 6969-6982.   \n\n\n\n__Changes w.r.t. version 1: renamed variables such that they match description.__\n\n\n\n\n\n### Dataset:  \n\nWilt Data Set \n\n\n\n### Abstract:    \n\nHigh-resolution Remote Sensing data set (Quickbird). Small number of training samples of diseased trees, large number for other land cover. Testing data set from stratified random sample of image.\n\n\n\n### Source:\n\n  \n\nBrian Johnson; \n\nInstitute for Global Environmental Strategies; \n\n2108-11 Kamiyamaguchi, Hayama, Kanagawa,240-0115 Japan; \n\nEmail: Johnson '@' iges.or.jp \n\n\n\n\n\n### Data Set Information:  \n\n\n\nThis data set contains some training and testing data from a remote sensing study by Johnson et al. (2013) that involved detecting diseased trees in Quickbird imagery. There are few training samples for the 'diseased trees' class (74) and many for 'other land cover' class (4265). \n\n\n\nThe data set consists of image segments, generated by segmenting the pansharpened image. The segments contain spectral information from the Quickbird multispectral image bands and texture information from the panchromatic (Pan) image band. The testing data set is for the row with \u00e2\u20ac\u0153Segmentation scale 15\u00e2\u20ac\u009d segments and \u00e2\u20ac\u0153original multi-spectral image\u00e2\u20ac\u009d Spectral information in Table 2 of the reference (i.e. row 5). Please see the reference below for more information on the data set, and please cite the reference if you use this data set. Enjoy! \n\n\n\n### Attribute Information:\n\n\n\nclass: 'w' (diseased trees), 'n' (all other land cover)   \n\nGLCM_Pan: GLCM mean texture (Pan band)   \n\nMean_G: Mean green value   \n\nMean_R: Mean red value   \n\nMean_NIR: Mean NIR value   \n\nSD_Pan: Standard deviation (Pan band)   \n\n\n\n\n\n### Relevant Papers:\n\n\n\nJohnson, B., Tateishi, R., Hoan, N., 2013. A hybrid pansharpening approach and multiscale object-based image analysis for mapping diseased pine and oak trees. International Journal of Remote Sensing, 34 (20), 6969-6982.", "numerai28.6": "**Author**: Numer.ai  \n\n**Source**: [Kaggle](https://www.kaggle.com/numerai/encrypted-stock-market-data-from-numerai)  \n\n**Please cite**:   \n\n\n\n**Encrypted Stock Market Training Data from Numer.ai**  \n\nThe data is cleaned, regularized and encrypted global equity data. The first 21 columns (feature1 - feature21) are features, and target is the binary class you\u2019re trying to predict.", "phoneme": "**Author**: Dominique Van Cappel, THOMSON-SINTRA  \n**Source**: [KEEL](http://sci2s.ugr.es/keel/dataset.php?cod=105#sub2), [ELENA](https://www.elen.ucl.ac.be/neural-nets/Research/Projects/ELENA/databases/REAL/phoneme/) - 1993  \n**Please cite**: None  \n\nThe aim of this dataset is to distinguish between nasal (class 0) and oral sounds (class 1). Five different attributes were chosen to characterize each vowel: they are the amplitudes of the five first harmonics AHi, normalised by the total energy Ene (integrated on all the frequencies): AHi/Ene. The phonemes are transcribed as follows: sh as in she, dcl as in dark, iy as the vowel in she, aa as the vowel in dark, and ao as the first vowel in water.  \n\n### Source\n\nThe current dataset was formatted by the KEEL repository, but originally hosted by the [ELENA Project](https://www.elen.ucl.ac.be/neural-nets/Research/Projects/ELENA/elena.htm#stuff). The dataset originates from the European ESPRIT 5516 project: ROARS. The aim of this project was the development and the implementation of a real time analytical system for French and Spanish speech recognition.  \n\n### Relevant information\n\nMost of the already existing speech recognition systems are global systems (typically Hidden Markov Models and Time Delay Neural Networks) which recognizes signals and do not really use the speech\nspecificities.  On the contrary, analytical systems take into account the articulatory process leading to the different phonemes of a given language, the idea being to deduce the presence of each of the\nphonetic features from the acoustic observation.\n\nThe main difficulty of analytical systems is to obtain acoustical parameters sufficiantly reliable. These acoustical measurements must :\n\n   - contain all the information relative to the concerned phonetic feature.\n   - being speaker independent.\n   - being context independent.\n   - being more or less robust to noise.\n\nThe primary acoustical observation is always voluminous (spectrum x N different observation moments) and classification cannot been processed directly.\n\nIn ROARS, the initial database is provided by cochlear spectra, which may be seen as the output of a filters bank having a constant DeltaF/F0, where the central frequencies are distributed on a\nlogarithmic scale (MEL type) to simulate the frequency answer of the auditory nerves.  The filters outputs are taken every 2 or 8 msec (integration on 4 or 16 msec) depending on the type of phoneme\nobserved (stationary or transitory).  \n\nThe aim of the present database is to distinguish between nasal and\noral vowels. There are thus two different classes:\n\n- Class 0 : Nasals  \n- Class 1 : Orals        \n\nThis database contains vowels coming from 1809 isolated syllables (for example: pa, ta, pan,...). Five different attributes were chosen to characterize each vowel: they are the amplitudes of the five first harmonics AHi, normalised by the total energy Ene (integrated on all the frequencies): AHi/Ene. Each harmonic is signed: positive when it corresponds to a local maximum of the spectrum and negative otherwise.\n\nThree observation moments have been kept for each vowel to obtain 5427 different instances: \n\n - the observation corresponding to the maximum total energy Ene. \n   \n - the observations taken 8 msec before and 8 msec after the observation corresponding to this maximum total energy.\n\nFrom these 5427 initial values, 23 instances for which the amplitude of the 5 first harmonics was zero were removed, leading to the 5404 instances of the present database. The patterns are presented in a random order.\n\n### Past Usage  \n\nAlinat, P., Periodic Progress Report 4, ROARS Project ESPRIT II- Number 5516, February 1993, Thomson report TS. ASM 93/S/EGS/NC/079  \n    \nGuerin-Dugue, A. and others, Deliverable R3-B4-P - Task B4: Benchmarks, Technical report, Elena-NervesII \"Enhanced Learning for Evolutive Neural Architecture\", ESPRIT-Basic Research Project  Number 6891, June 1995  \n\nVerleysen, M. and Voz, J.L. and Thissen, P. and Legat, J.D., A statistical Neural Network for high-dimensional vector classification, ICNN'95 - IEEE International Conference on Neural Networks, November 1995, Perth, Western Australia.  \n    \nVoz J.L., Verleysen M., Thissen P. and Legat J.D., Suboptimal Bayesian classification by vector quantization with small clusters. ESANN95-European Symposium on Artificial Neural Networks, April 1995, M. Verleysen editor, D facto publications, Brussels, Belgium.  \n    \nVoz J.L., Verleysen M., Thissen P. and Legat J.D., A practical view of  suboptimal Bayesian classification, IWANN95-Proceedings of the International Workshop on Artificial Neural Networks, June 1995, Mira, Cabestany, Prieto editors, Springer-Verlag Lecture Notes in Computer Sciences, Malaga, Spain", "credit-g": "**Author**: Dr. Hans Hofmann  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) - 1994    \n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)\n\n**German Credit dataset**  \nThis dataset classifies people described by a set of attributes as good or bad credit risks.\n\nThis dataset comes with a cost matrix: \n``` \nGood  Bad (predicted)  \nGood   0    1   (actual)  \nBad    5    0  \n```\n\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).  \n\n### Attribute description  \n\n1. Status of existing checking account, in Deutsche Mark.  \n2. Duration in months  \n3. Credit history (credits taken, paid back duly, delays, critical accounts)  \n4. Purpose of the credit (car, television,...)  \n5. Credit amount  \n6. Status of savings account/bonds, in Deutsche Mark.  \n7. Present employment, in number of years.  \n8. Installment rate in percentage of disposable income  \n9. Personal status (married, single,...) and sex  \n10. Other debtors / guarantors  \n11. Present residence since X years  \n12. Property (e.g. real estate)  \n13. Age in years  \n14. Other installment plans (banks, stores)  \n15. Housing (rent, own,...)  \n16. Number of existing credits at this bank  \n17. Job  \n18. Number of people being liable to provide maintenance for  \n19. Telephone (yes,no)  \n20. Foreign worker (yes,no)", "steel-plates-fault": "**Author**: Semeion, Research Center of Sciences of Communication, Rome, Italy.     \n\n**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/steel+plates+faults)     \n\n**Please cite**: Dataset provided by Semeion, Research Center of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy.  \n\n\n\n__Changes w.r.t. version 1: included one target factor with 7 levels as target variable for the classification. Also deleted the previous 7 binary target variables.__\n\n\n\n**Steel Plates Faults Data Set**  \n\nA dataset of steel plates' faults, classified into 7 different types. The goal was to train machine learning for automatic pattern recognition.\n\n\n\nThe dataset consists of 27 features describing each fault (location, size, ...) and 1 feature indicating the type of fault (on of 7: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults). The target is the type of fault.\n\n\n\n### Attribute Information  \n\n* V1: X_Minimum  \n\n* V2: X_Maximum  \n\n* V3: Y_Minimum  \n\n* V4: Y_Maximum  \n\n* V5: Pixels_Areas  \n\n* V6: X_Perimeter  \n\n* V7: Y_Perimeter  \n\n* V8: Sum_of_Luminosity  \n\n* V9: Minimum_of_Luminosity  \n\n* V10: Maximum_of_Luminosity  \n\n* V11: Length_of_Conveyer  \n\n* V12: TypeOfSteel_A300  \n\n* V13: TypeOfSteel_A400  \n\n* V14: Steel_Plate_Thickness  \n\n* V15: Edges_Index  \n\n* V16: Empty_Index  \n\n* V17: Square_Index  \n\n* V18: Outside_X_Index  \n\n* V19: Edges_X_Index  \n\n* V20: Edges_Y_Index  \n\n* V21: Outside_Global_Index  \n\n* V22: LogOfAreas  \n\n* V23: Log_X_Index  \n\n* V24: Log_Y_Index  \n\n* V25: Orientation_Index  \n\n* V26: Luminosity_Index  \n\n* V27: SigmoidOfAreas  \n\n* target: 7 types of fault as classification target  \n\n\n\n### Relevant Papers  \n\n1.M Buscema, S Terzi, W Tastle, A New Meta-Classifier,in NAFIPS 2010, Toronto (CANADA),26-28 July 2010, 978-1-4244-7858-6/10 \u00c2\u00a92010 IEEE  \n\n2.M Buscema, MetaNet: The Theory of Independent Judges, in Substance Use & Misuse, 33(2), 439-461,1998", "APSFailure": "This is the dataset used for the 2016 IDA Industrial Challenge, courtesy of Scania.\n\nFor a full description, see http://archive.ics.uci.edu/ml/datasets/IDA2016Challenge .\n\nThis dataset contains both the train and test set provided.\n\nThe first 60000 samples are the train set, and the last 16000 samples are\n\nthe test set.\n\n\n\nData was published under the GNU GPL v3 license.", "dilbert": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "fabert": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "jasmine": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "airlines": "**Author**: Albert Bifet, Elena Ikonomovska  \n**Source**: [Data Expo competition](http://kt.ijs.si/elena_ikonomovska/data.html) - 2009  \n**Please cite**:   \n\nAirlines Dataset Inspired in the regression dataset from Elena Ikonomovska. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.", "dionis": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "albert": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "gina": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "ozone-level-8hr": "**Author**: Kun Zhang, Wei Fan, XiaoJing Yuan\n\n\n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/ozone+level+detection)\n\n\n\n**Please cite**:   \n\n\n\nForecasting skewed biased stochastic ozone days: analyses, solutions and beyond, Knowledge and Information Systems, Vol. 14, No. 3, 2008. \n\n\n\n\n\n1 . Abstract: \n\nTwo ground ozone level data sets are included in this collection. One is the eight hour peak set (eighthr.data), the other is the one hour peak set (onehr.data). Those data were collected from 1998 to 2004 at the Houston, Galveston and Brazoria area.\n\n\n\n2. Source:\n\n\n\nKun Zhang, zhang.kun05 '@' gmail.com, Department of Computer Science, Xavier University of Lousiana \n\nWei Fan, wei.fan '@' gmail.com, IBM T.J.Watson Research \n\nXiaoJing Yuan, xyuan '@' uh.edu, Engineering Technology Department, College of Technology, University of Houston \n\n\n\n\n\n3. Data Set Information:\n\n\n\nAll the attribute start with T means the temperature measured at different time throughout the day; and those starts with WS indicate the wind speed at various time. \n\n\n\nWSR_PK: continuous. peek wind speed -- resultant (meaning average of wind vector) \n\nWSR_AV: continuous. average wind speed \n\nT_PK: continuous. Peak T \n\nT_AV: continuous. Average T \n\nT85: continuous. T at 850 hpa level (or about 1500 m height) \n\nRH85: continuous. Relative Humidity at 850 hpa \n\nU85: continuous. (U wind - east-west direction wind at 850 hpa) \n\nV85: continuous. V wind - N-S direction wind at 850 \n\nHT85: continuous. Geopotential height at 850 hpa, it is about the same as height at low altitude \n\nT70: continuous. T at 700 hpa level (roughly 3100 m height) \n\n\n\nRH70: continuous. \n\nU70: continuous. \n\nV70: continuous. \n\nHT70: continuous. \n\n\n\nT50: continuous. T at 500 hpa level (roughly at 5500 m height) \n\n\n\nRH50: continuous. \n\nU50: continuous. \n\nV50: continuous. \n\nHT50: continuous. \n\n\n\nKI: continuous. K-Index [Web Link] \n\nTT: continuous. T-Totals [Web Link] \n\nSLP: continuous. Sea level pressure \n\nSLP_: continuous. SLP change from previous day \n\n\n\nPrecp: continuous. -- precipitation\n\n\n\n\n\n4. Attribute Information:\n\n\n\nThe following are specifications for several most important attributes that are highly valued by Texas Commission on Environmental Quality (TCEQ). More details can be found in the two relevant papers. \n\n\n\nO 3 - Local ozone peak prediction \n\nUpwind - Upwind ozone background level \n\nEmFactor - Precursor emissions related factor \n\nTmax - Maximum temperature in degrees F \n\nTb - Base temperature where net ozone production begins (50 F) \n\nSRd - Solar radiation total for the day \n\nWSa - Wind speed near sunrise (using 09-12 UTC forecast mode) \n\nWSp - Wind speed mid-day (using 15-21 UTC forecast mode) \n\n\n\n\n\n5. Relevant Papers:\n\n\n\nForecasting skewed biased stochastic ozone days: analyses, solutions and beyond, Knowledge and Information Systems, Vol. 14, No. 3, 2008. \n\n\n\nIt Discusses details about the dataset, its use as well as various experiments (both cross-validation and streaming) using many state-of-the-art methods. \n\nA shorter version of the paper (does not contain some detailed experiments as the journal paper above) is in: \n\nForecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions. ICDM 2006: 753-764", "vehicle": "**Author**: Dr. Pete Mowforth and Dr. Barry Shepherd  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes))\n\n**Please cite**: Siebert,JP. Turing Institute Research Memorandum TIRM-87-018 \"Vehicle Recognition Using Rule Based Methods\" (March 1987)  \n\n\n\n NAME\n\n         vehicle silhouettes\n\n \n\n PURPOSE\n\n         to classify a given silhouette as one of four types of vehicle,\n\n         using  a set of features extracted from the silhouette. The\n\n         vehicle may be viewed from one of many different angles.  \n\n \n\n PROBLEM TYPE\n\n         classification\n\n         \n\n SOURCE\n\n         Drs.Pete Mowforth and Barry Shepherd\n\n         Turing Institute\n\n         George House\n\n         36 North Hanover St.\n\n         Glasgow\n\n         G1 2AD\n\n \n\n CONTACT\n\n         Alistair Sutherland\n\n         Statistics Dept.\n\n         Strathclyde University\n\n         Livingstone Tower\n\n         26 Richmond St.\n\n         GLASGOW G1 1XH\n\n         Great Britain\n\n         \n\n         Tel: 041 552 4400 x3033\n\n         \n\n         Fax: 041 552 4711 \n\n         \n\n         e-mail: alistair@uk.ac.strathclyde.stams\n\n \n\n HISTORY\n\n         This data was originally gathered at the TI in 1986-87 by\n\n         JP Siebert. It was partially financed by Barr and Stroud Ltd.\n\n         The original purpose was to find a method of distinguishing\n\n         3D objects within a 2D image by application of an ensemble of\n\n         shape feature extractors to the 2D silhouettes of the objects.\n\n         Measures of shape features extracted from example silhouettes\n\n         of objects to be discriminated were used to generate a class-\n\n         ification rule tree by means of computer induction.\n\n          This object recognition strategy was successfully used to \n\n         discriminate between silhouettes of model cars, vans and buses\n\n         viewed from constrained elevation but all angles of rotation.\n\n          The rule tree classification performance compared favourably\n\n         to MDC (Minimum Distance Classifier) and k-NN (k-Nearest Neigh-\n\n         bour) statistical classifiers in terms of both error rate and\n\n         computational efficiency. An investigation of these rule trees\n\n         generated by example indicated that the tree structure was \n\n         heavily influenced by the orientation of the objects, and grouped\n\n         similar object views into single decisions.\n\n \n\n DESCRIPTION\n\n          The features were extracted from the silhouettes by the HIPS\n\n         (Hierarchical Image Processing System) extension BINATTS, which \n\n         extracts a combination of scale independent features utilising\n\n         both classical moments based measures such as scaled variance,\n\n         skewness and kurtosis about the major/minor axes and heuristic\n\n         measures such as hollows, circularity, rectangularity and\n\n         compactness.\n\n          Four \"Corgie\" model vehicles were used for the experiment:\n\n         a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400.\n\n         This particular combination of vehicles was chosen with the \n\n         expectation that the bus, van and either one of the cars would\n\n         be readily distinguishable, but it would be more difficult to\n\n         distinguish between the cars.\n\n          The images were acquired by a camera looking downwards at the\n\n         model vehicle from a fixed angle of elevation (34.2 degrees\n\n         to the horizontal). The vehicles were placed on a diffuse\n\n         backlit surface (lightbox). The vehicles were painted matte black\n\n         to minimise highlights. The images were captured using a CRS4000\n\n         framestore connected to a vax 750. All images were captured with\n\n         a spatial resolution of 128x128 pixels quantised to 64 greylevels.\n\n         These images were thresholded to produce binary vehicle silhouettes,\n\n         negated (to comply with the processing requirements of BINATTS) and\n\n         thereafter subjected to shrink-expand-expand-shrink HIPS modules to\n\n         remove \"salt and pepper\" image noise.\n\n          The vehicles were rotated and their angle of orientation was measured\n\n         using a radial graticule beneath the vehicle. 0 and 180 degrees\n\n         corresponded to \"head on\" and \"rear\" views respectively while 90 and\n\n         270 corresponded to profiles in opposite directions. Two sets of\n\n         60 images, each set covering a full 360 degree rotation, were captured\n\n         for each vehicle. The vehicle was rotated by a fixed angle between \n\n         images. These datasets are known as e2 and e3 respectively.\n\n          A further two sets of images, e4 and e5, were captured with the camera \n\n         at elevations of 37.5 degs and 30.8 degs respectively. These sets\n\n         also contain 60 images per vehicle apart from e4.van which contains\n\n         only 46 owing to the difficulty of containing the van in the image\n\n         at some orientations.\n\n \n\n ATTRIBUTES\n\n         \n\n         COMPACTNESS     (average perim)**2/area\n\n         \n\n         CIRCULARITY     (average radius)**2/area\n\n         \n\n         DISTANCE CIRCULARITY    area/(av.distance from border)**2\n\n         \n\n         RADIUS RATIO    (max.rad-min.rad)/av.radius\n\n         \n\n         PR.AXIS ASPECT RATIO    (minor axis)/(major axis)\n\n         \n\n         MAX.LENGTH ASPECT RATIO (length perp. max length)/(max length)\n\n         \n\n         SCATTER RATIO   (inertia about minor axis)/(inertia about major axis)\n\n         \n\n         ELONGATEDNESS           area/(shrink width)**2\n\n         \n\n         PR.AXIS RECTANGULARITY  area/(pr.axis length*pr.axis width)\n\n         \n\n         MAX.LENGTH RECTANGULARITY area/(max.length*length perp. to this)\n\n         \n\n         SCALED VARIANCE         (2nd order moment about minor axis)/area\n\n         ALONG MAJOR AXIS\n\n         \n\n         SCALED VARIANCE         (2nd order moment about major axis)/area\n\n         ALONG MINOR AXIS \n\n         \n\n         SCALED RADIUS OF GYRATION       (mavar+mivar)/area\n\n         \n\n         SKEWNESS ABOUT  (3rd order moment about major axis)/sigma_min**3\n\n         MAJOR AXIS\n\n         \n\n         SKEWNESS ABOUT  (3rd order moment about minor axis)/sigma_maj**3\n\n         MINOR AXIS\n\n                 \n\n         KURTOSIS ABOUT  (4th order moment about major axis)/sigma_min**4\n\n         MINOR AXIS  \n\n                 \n\n         KURTOSIS ABOUT  (4th order moment about minor axis)/sigma_maj**4\n\n         MAJOR AXIS\n\n         \n\n         HOLLOWS RATIO   (area of hollows)/(area of bounding polygon)\n\n         \n\n          Where sigma_maj**2 is the variance along the major axis and\n\n         sigma_min**2 is the variance along the minor axis, and\n\n         \n\n         area of hollows= area of bounding poly-area of object \n\n         \n\n          The area of the bounding polygon is found as a side result of\n\n         the computation to find the maximum length. Each individual\n\n         length computation yields a pair of calipers to the object\n\n         orientated at every 5 degrees. The object is propagated into\n\n         an image containing the union of these calipers to obtain an\n\n         image of the bounding polygon. \n\n         \n\n NUMBER OF CLASSES\n\n \n\n         4       OPEL, SAAB, BUS, VAN\n\n \n\n NUMBER OF EXAMPLES\n\n \n\n                 Total no. = 946\n\n                 \n\n                 No. in each class\n\n                 \n\n                   opel 240\n\n                   saab 240\n\n                   bus  240\n\n                   van  226\n\n                 \n\n                 \n\n                 100 examples are being kept by Strathclyde for validation.\n\n                 So StatLog partners will receive 846 examples.\n\n \n\n NUMBER OF ATTRIBUTES\n\n \n\n                 No. of atts. = 18", "madeline": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "philippine": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "ada": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "arcene": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "jannis": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "Diabetes130US": "**Author**: Attila Reiss, Department Augmented Vision, DFKI, Germany, \"attila.reiss '@' dfki.de  \n\n**Date**: August 2012.  \n\n**Source**: UCI  \n\n**Please cite**: Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore, &ldquo; Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records,&rdquo; BioMed Research International, vol. 2014, Article ID 781670, 11 pages, 2014.  \n\n\n\nThis data has been prepared to analyze factors related to readmission as well as other outcomes pertaining to patients with diabetes.\n\n\n\n**Source**  \n\nThe data are submitted on behalf of the Center for Clinical and Translational Research, Virginia Commonwealth University, a recipient of NIH CTSA grant UL1 TR00058 and a recipient of the CERNER data. John Clore (jclore '@' vcu.edu), Krzysztof J. Cios (kcios '@' vcu.edu), Jon DeShazo (jpdeshazo '@' vcu.edu), and Beata Strack (strackb '@' vcu.edu). This data is a de-identified abstract of the Health Facts database (Cerner Corporation, Kansas City, MO).\n\n\n\n**Data Set Information**  \n\nThe dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria:  \n\n(1) It is an inpatient encounter (a hospital admission).  \n\n(2) It is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.  \n\n(3) The length of stay was at least 1 day and at most 14 days.  \n\n(4) Laboratory tests were performed during the encounter.  \n\n(5) Medications were administered during the encounter.  \n\nThe data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.\n\n\n\n**Attribute Information**  \n\nDetailed description of all the attributes is provided in Table 1 of the paper.  \n\n\n\n**Relevant Papers**  \n\nBeata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore, &ldquo;Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records,&rdquo; BioMed Research International, vol. 2014, Article ID 781670, 11 pages, 2014.\n\n\n\n[Web Link](https://www.hindawi.com/journals/bmri/2014/781670/)", "micro-mass": "**Author**: Pierre Mah\u00e9, Jean-Baptiste Veyrieras  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/MicroMass) - 2014  \n\n**Please cite**:   \n\n\n\n### Description\n\n\n\nMicroMass (pure spectra version) is a dataset to explore machine learning approaches for the identification of microorganisms from mass-spectrometry data.  \n\n\n\n### Source\n\n```\n\nPierre Mah\u00e9, pierre.mahe '@' biomerieux.com, bioM\u00e9rieux\n\nJean-Baptiste Veyrieras, jean-baptiste.veyrieras '@' biomerieux.com, bioM\u00e9rieux\n\n```\n\n\n\n### Data Set Information\n\n\n\nThis MALDI-TOF dataset consists in:\n\n\n\na) A reference panel of 20 Gram positive and negative bacterial species covering 9 genera among which several species are known to be hard to discriminate by mass spectrometry (MALDI-TOF). Each species was represented by 11 to 60 mass spectra obtained from 7 to 20 bacterial strains, constituting altogether a dataset of 571 spectra obtained from 213 strains. The spectra were obtained according to the standard culture-based workflow used in clinical routine in which the microorganism was first grown on an agar plate for 24 to 48 hours before a portion of the colony was picked, spotted on a MALDI slide and a mass spectrum was acquired. \n\n\n\nb) Based on this reference panel, a dedicated in vitro mock-up mixture dataset was constituted. For that purpose we considered 10 pairs of species of various taxonomic proximity:\n\n* 4 mixtures, labeled A, B, C and D, involved species that belong to the same genus,  \n\n* 2 mixtures, labeled E and F, involved species that belong to distinct genera, but to the same Gram type,  \n\n* 4 mixtures, labeled G, H, I and J, involved species that belong to distinct Gram types.  \n\nEach mixture was represented by 2 pairs of strains, which were mixed according to the following 9 concentration ratios : 1:0, 10:1, 5:1, 2:1, 1:1, 1:2, 1:5, 1:10, 0:1. Two replicate spectra were acquired for each concentration ratio and each couple of strains, leading altogether to a dataset of 360 spectra, among which 80 are actually pure sample spectra.\n\n\n\n### Relevant Papers\n\n\n\nMah\u00e9 et al. (2014). Automatic identification of mixed bacterial species fingerprints in a MALDI-TOF mass-spectrum. Bioinformatics.\n\n\n\nVervier et al., A benchmark of support vector machines strategies for microbial identification by mass-spectrometry data, submitted", "eucalyptus": "**Author**: Bruce Bulloch    \n\n**Source**: [WEKA Dataset Collection](http://www.cs.waikato.ac.nz/ml/weka/datasets.html) - part of the agridatasets archive. [This is the true source](http://tunedit.org/repo/Data/Agricultural/eucalyptus.arff)  \n\n**Please cite**: None  \n\n\n\n**Eucalyptus Soil Conservation**  \n\nThe objective was to determine which seedlots in a species are best for soil conservation in seasonally dry hill country. Determination is found by measurement of height, diameter by height, survival, and other contributing factors. \n\n \n\nIt is important to note that eucalypt trial methods changed over time; earlier trials included mostly 15 - 30cm tall seedling grown in peat plots and the later trials have included mostly three replications of eight trees grown. This change may contribute to less significant results.\n\n\n\nExperimental data recording procedures which require noting include:\n\n - instances with no data recorded due to experimental recording procedures\n\n   require that the absence of a species from one replicate at a site was\n\n   treated as a missing value, but if absent from two or more replicates at a\n\n   site the species was excluded from the site's analyses.\n\n - missing data for survival, vigour, insect resistance, stem form, crown form\n\n   and utility especially for the data recorded at the Morea Station; this \n\n   could indicate the death of species in these areas or a lack in collection\n\n   of data.  \n\n\n\n### Attribute Information  \n\n \n\n  1.  Abbrev - site abbreviation - enumerated\n\n  2.  Rep - site rep - integer\n\n  3.  Locality - site locality in the North Island - enumerated\n\n  4.  Map_Ref - map location in the North Island - enumerated\n\n  5.  Latitude - latitude approximation - enumerated\n\n  6.  Altitude - altitude approximation - integer\n\n  7.  Rainfall - rainfall (mm pa) - integer\n\n  8.  Frosts - frosts (deg. c) - integer\n\n  9.  Year - year of planting - integer\n\n  10. Sp - species code - enumerated\n\n  11. PMCno - seedlot number - integer\n\n  12. DBH - best diameter base height (cm) - real\n\n  13. Ht - height (m) - real\n\n  14. Surv - survival - integer\n\n  15. Vig - vigour - real\n\n  16. Ins_res - insect resistance - real\n\n  17. Stem_Fm - stem form - real\n\n  18. Crown_Fm - crown form - real\n\n  19. Brnch_Fm - branch form - real\n\n  Class:\n\n  20. Utility - utility rating - enumerated\n\n\n\n### Relevant papers\n\n\n\nBulluch B. T., (1992) Eucalyptus Species Selection for Soil Conservation in Seasonally Dry Hill Country - Twelfth Year Assessment  New Zealand Journal of Forestry Science 21(1): 10 - 31 (1991)  \n\n\n\nKirsten Thomson and Robert J. McQueen (1996) Machine Learning Applied to Fourteen Agricultural Datasets. University of Waikato Research Report  \n\nhttps://www.cs.waikato.ac.nz/ml/publications/1996/Thomson-McQueen-96.pdf + the original publication:", "blood-transfusion-service-center": "**Author**: Prof. I-Cheng Yeh  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center)  \n**Please cite**: Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming, \"Knowledge discovery on RFM model using Bernoulli sequence\", Expert Systems with Applications, 2008.   \n\n**Blood Transfusion Service Center Data Set**  \nData taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan -- this is a classification problem.\n\nTo demonstrate the RFMTC marketing model (a modified version of RFM), this study adopted the donor database of Blood Transfusion Service Center in Hsin-Chu City in Taiwan. The center passes their blood transfusion service bus to one university in Hsin-Chu City to gather blood donated about every three months. To build an FRMTC model, we selected 748 donors at random from the donor database. \n\n### Attribute Information  \n* V1: Recency - months since last donation\n* V2: Frequency - total number of donation\n* V3: Monetary - total blood donated in c.c.\n* V4: Time - months since first donation), and a binary variable representing whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood).\n\nThe target attribute is a binary variable representing whether he/she donated blood in March 2007 (2 stands for donating blood; 1 stands for not donating blood).", "qsar-biodeg": "**Author**: Kamel Mansouri, Tine Ringsted, Davide Ballabio  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation)  \n\n**Please cite**: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878 \n\n\n\n\n\nQSAR biodegradation Data Set \n\n\n\n* Abstract: \n\n\n\nData set containing values for 41 attributes (molecular descriptors) used to classify 1055 chemicals into 2 classes (ready and not ready biodegradable).\n\n\n\n\n\n* Source:\n\n\n\nKamel Mansouri, Tine Ringsted, Davide Ballabio (davide.ballabio '@' unimib.it), Roberto Todeschini, Viviana Consonni, Milano Chemometrics and QSAR Research Group (http://michem.disat.unimib.it/chm/), Universit\u00c3  degli Studi Milano \u00e2\u20ac\u201c Bicocca, Milano (Italy)\n\n\n\n\n\n* Data Set Information:\n\n\n\nThe QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (Universit\u00c3  degli Studi Milano \u00e2\u20ac\u201c Bicocca, Milano, Italy). The research leading to these results has received funding from the European Community\u00e2\u20ac\u2122s Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project. \n\nThe data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.\n\n\n\n\n\n* Attribute Information:\n\n\n\n41 molecular descriptors and 1 experimental class: \n\n1) SpMax_L: Leading eigenvalue from Laplace matrix \n\n2) J_Dz(e): Balaban-like index from Barysz matrix weighted by Sanderson electronegativity \n\n3) nHM: Number of heavy atoms \n\n4) F01[N-N]: Frequency of N-N at topological distance 1 \n\n5) F04[C-N]: Frequency of C-N at topological distance 4 \n\n6) NssssC: Number of atoms of type ssssC \n\n7) nCb-: Number of substituted benzene C(sp2) \n\n8) C%: Percentage of C atoms \n\n9) nCp: Number of terminal primary C(sp3) \n\n10) nO: Number of oxygen atoms \n\n11) F03[C-N]: Frequency of C-N at topological distance 3 \n\n12) SdssC: Sum of dssC E-states \n\n13) HyWi_B(m): Hyper-Wiener-like index (log function) from Burden matrix weighted by mass \n\n14) LOC: Lopping centric index \n\n15) SM6_L: Spectral moment of order 6 from Laplace matrix \n\n16) F03[C-O]: Frequency of C - O at topological distance 3 \n\n17) Me: Mean atomic Sanderson electronegativity (scaled on Carbon atom) \n\n18) Mi: Mean first ionization potential (scaled on Carbon atom) \n\n19) nN-N: Number of N hydrazines \n\n20) nArNO2: Number of nitro groups (aromatic) \n\n21) nCRX3: Number of CRX3 \n\n22) SpPosA_B(p): Normalized spectral positive sum from Burden matrix weighted by polarizability \n\n23) nCIR: Number of circuits \n\n24) B01[C-Br]: Presence/absence of C - Br at topological distance 1 \n\n25) B03[C-Cl]: Presence/absence of C - Cl at topological distance 3 \n\n26) N-073: Ar2NH / Ar3N / Ar2N-Al / R..N..R \n\n27) SpMax_A: Leading eigenvalue from adjacency matrix (Lovasz-Pelikan index) \n\n28) Psi_i_1d: Intrinsic state pseudoconnectivity index - type 1d \n\n29) B04[C-Br]: Presence/absence of C - Br at topological distance 4 \n\n30) SdO: Sum of dO E-states \n\n31) TI2_L: Second Mohar index from Laplace matrix \n\n32) nCrt: Number of ring tertiary C(sp3) \n\n33) C-026: R--CX--R \n\n34) F02[C-N]: Frequency of C - N at topological distance 2 \n\n35) nHDon: Number of donor atoms for H-bonds (N and O) \n\n36) SpMax_B(m): Leading eigenvalue from Burden matrix weighted by mass \n\n37) Psi_i_A: Intrinsic state pseudoconnectivity index - type S average \n\n38) nN: Number of Nitrogen atoms \n\n39) SM6_B(m): Spectral moment of order 6 from Burden matrix weighted by mass \n\n40) nArCOOR: Number of esters (aromatic) \n\n41) nX: Number of halogen atoms \n\n42) experimental class: ready biodegradable (RB) and not ready biodegradable (NRB)\n\n\n\n\n\n* Relevant Papers:\n\n\n\nMansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878", "cnae-9": "**Author**: Patrick Marques Ciarelli, Elias Oliviera   \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/CNAE-9) - 2010  \n\n**Please cite**:   \n\n\n\n### Description\n\n\n\nThis is a data set containing 1080 documents of free text business descriptions of Brazilian companies categorized into a subset of 9 categories.\n\n\n\n### Source\n\n```\n\nPatrick Marques Ciarelli, pciarelli '@' lcad.inf.ufes.br, Department of Electrical Engineering, Federal University of Espirito Santo \n\nElias Oliveira, elias '@' lcad.inf.ufes.br, Department of Information Science, Federal University of Espirito Santo\n\n```\n\n\n\n### Data Set Information\n\n\n\nThis is a data set containing 1080 documents of free text business descriptions of Brazilian companies categorized into a \n\nsubset of 9 categories cataloged in a table called National Classification of Economic Activities (Classifica\u00e7\u00e3o Nacional de \n\nAtividade Econ\u00f4micas - CNAE). The original texts were preprocessed to obtain the current data set: initially, it was kept only letters and then it was removed prepositions of the texts. Next, the words were transformed to their canonical form. Finally, \n\neach document was represented as a vector, where the weight of each word is its frequency in the document. \n\nThis data set is highly sparse (99.22% of the matrix is filled with zeros).\n\n\n\n### Attribute Information\n\n\n\nIn the dataset there are 857 attributes, 1 attributes with the class of instance and 856 with word frequency:\n\n ```\n\n1. category: range 1 - 9 (integer)   \n\n2. 857. word frequency: (integer)\n\n```\n\n\n\n### Relevant Papers\n\n\n\nPatrick Marques Ciarelli, Elias Oliveira, 'Agglomeration and Elimination of Terms for Dimensionality Reduction', \n\nNinth International Conference on Intelligent Systems Design and Applications, pp.547-552, 2009 \n\n\n\nPatrick Marques Ciarelli, Elias Oliveira, Evandro O. T. Salles, 'An Evolving System Based on Probabilistic Neural Network', \n\nBrazilian Symposium on Artificial Neural Network, 2010", "pc4": "**Author**: Mike Chapman, NASA  \n\n**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/pc1.html) - 2004  \n\n**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n\n  \n\n**PC4 Software defect prediction**  \n\nOne of the NASA Metrics Data Program defect data sets. Data from flight software for earth orbiting satellite. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n\n\n\n### Relevant papers  \n\n\n\n- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n\nData Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n\n\n\n- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n\nSoftware Engineering.\n\n\n\n- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago", "cmc": "**Author**: [Tjen-Sien Lim](limt@stat.wisc.edu) \n\n**Source**: [As obtained from UCI](https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice)\n\n**Please cite**: [UCI citation](https://archive.ics.uci.edu/ml/citation_policy.html)\n\n\n\n1. Title: Contraceptive Method Choice\n\n \n\n 2. Sources:\n\n    (a) Origin:  This dataset is a subset of the 1987 National Indonesia\n\n                 Contraceptive Prevalence Survey\n\n    (b) Creator: Tjen-Sien Lim (limt@stat.wisc.edu)\n\n    (c) Donor:   Tjen-Sien Lim (limt@stat.wisc.edu)\n\n    (c) Date:    June 7, 1997\n\n \n\n 3. Past Usage:\n\n    Lim, T.-S., Loh, W.-Y. & Shih, Y.-S. (1999). A Comparison of\n\n    Prediction Accuracy, Complexity, and Training Time of Thirty-three\n\n    Old and New Classification Algorithms. Machine Learning. Forthcoming.\n\n    (ftp://ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or\n\n    (http://www.stat.wisc.edu/~limt/mach1317.pdf)\n\n \n\n 4. Relevant Information:\n\n    This dataset is a subset of the 1987 National Indonesia Contraceptive\n\n    Prevalence Survey. The samples are married women who were either not \n\n    pregnant or do not know if they were at the time of interview. The \n\n    problem is to predict the current contraceptive method choice \n\n    (no use, long-term methods, or short-term methods) of a woman based \n\n    on her demographic and socio-economic characteristics.\n\n \n\n 5. Number of Instances: 1473\n\n \n\n 6. Number of Attributes: 10 (including the class attribute)\n\n \n\n 7. Attribute Information:\n\n \n\n    1. Wife's age                     (numerical)\n\n    2. Wife's education               (categorical)      1=low, 2, 3, 4=high\n\n    3. Husband's education            (categorical)      1=low, 2, 3, 4=high\n\n    4. Number of children ever born   (numerical)\n\n    5. Wife's religion                (binary)           0=Non-Islam, 1=Islam\n\n    6. Wife's now working?            (binary)           0=Yes, 1=No\n\n    7. Husband's occupation           (categorical)      1, 2, 3, 4\n\n    8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n\n    9. Media exposure                 (binary)           0=Good, 1=Not good\n\n    10. Contraceptive method used     (class attribute)  1=No-use \n\n                                                         2=Long-term\n\n                                                         3=Short-term\n\n \n\n 8. Missing Attribute Values: None\n\n\n\n Information about the dataset\n\n CLASSTYPE: nominal\n\n CLASSINDEX: last", "car": "**Author**: Marko Bohanec, Blaz Zupan  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/car+evaluation) - 1997   \n**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)  \n\n**Car Evaluation Database**  \nThis database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX (M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.).\n\nThe model evaluates cars according to the following concept structure:\n \n    CAR                      car acceptability\n    . PRICE                  overall price\n    . . buying               buying price\n    . . maint                price of the maintenance\n    . TECH                   technical characteristics\n    . . COMFORT              comfort\n    . . . doors              number of doors\n    . . . persons            capacity in terms of persons to carry\n    . . . lug_boot           the size of luggage boot\n    . . safety               estimated safety of the car\n \nInput attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for\nthese examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\n \nThe Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety. Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\n\n### Changes with respect to car (1)\n\nThe ordinal variables are stored as ordered factors in this version. \n\n### Relevant papers:  \nM. Bohanec and V. Rajkovic: Knowledge acquisition and explanation for multi-attribute decision making. In 8th Intl Workshop on Expert Systems and their Applications, Avignon, France. pages 59-78, 1988.  \n\nM. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.", "mfeat-factors": "**Author**: Robert P.W. Duin, Department of Applied Physics, Delft University of Technology  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Multiple+Features) - 1998  \n\n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)   \n\n\n\n**Multiple Features Dataset: Factors**  \n\nOne of a set of 6 datasets describing features of handwritten numerals (0 - 9) extracted from a collection of Dutch utility maps. Corresponding patterns in different datasets correspond to the same original character. 200 instances per class (for a total of 2,000 instances) have been digitized in binary images. \n\n\n\n### Attribute Information  \n\nThe attributes represent 216 profile correlations. No more information is known.\n\n\n\n### Relevant Papers  \n\nA slightly different version of the database is used in  \n\nM. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog, Handwritten digit recognition by combined classifiers, Kybernetika, vol. 34, no. 4, 1998, 381-386.\n\n \n\nThe database as is is used in:  \n\nA.K. Jain, R.P.W. Duin, J. Mao, Statistical Pattern Recognition: A Review, IEEE Transactions on Pattern Analysis and Machine Intelligence archive, Volume 22 Issue 1, January 2000", "kc1": "**Author**: Mike Chapman, NASA  \n\n**Source**: [tera-PROMISE](http://openscience.us/repo/defect/mccabehalsted/kc1.html) - 2004  \n\n**Please cite**: Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  \n\n  \n\n**KC1 Software defect prediction**  \n\nOne of the NASA Metrics Data Program defect data sets. Data from software for storage management for receiving and processing ground data. Data comes from McCabe and Halstead features extractors of source code.  These features were defined in the 70s in an attempt to objectively characterize code features that are associated with software quality.\n\n\n\n### Attribute Information  \n\n\n\n1. loc             : numeric % McCabe's line count of code\n\n2. v(g)            : numeric % McCabe \"cyclomatic complexity\"\n\n3. ev(g)           : numeric % McCabe \"essential complexity\"\n\n4. iv(g)           : numeric % McCabe \"design complexity\"\n\n5. n               : numeric % Halstead total operators + operands\n\n6. v               : numeric % Halstead \"volume\"\n\n7. l               : numeric % Halstead \"program length\"\n\n8. d               : numeric % Halstead \"difficulty\"\n\n9. i               : numeric % Halstead \"intelligence\"\n\n10. e               : numeric % Halstead \"effort\"\n\n11. b               : numeric % Halstead \n\n12. t               : numeric % Halstead's time estimator\n\n13. lOCode          : numeric % Halstead's line count\n\n14. lOComment       : numeric % Halstead's count of lines of comments\n\n15. lOBlank         : numeric % Halstead's count of blank lines\n\n16. lOCodeAndComment: numeric\n\n17. uniq_Op         : numeric % unique operators\n\n18. uniq_Opnd       : numeric % unique operands\n\n19. total_Op        : numeric % total operators\n\n20. total_Opnd      : numeric % total operands\n\n21. branchCount     : numeric % of the flow graph\n\n22. problems        : {false,true} % module has/has not one or more reported defects\n\n\n\n### Relevant papers  \n\n\n\n- Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C. (2013)\n\nData Quality: Some Comments on the NASA Software Defect Datasets, IEEE Transactions on Software Engineering, 39.\n\n\n\n- Tim Menzies and Justin S. Di Stefano (2004) How Good is Your Blind Spot Sampling Policy? 2004 IEEE Conference on High Assurance\n\nSoftware Engineering.\n\n\n\n- T. Menzies and J. DiStefano and A. Orrego and R. Chapman (2004) Assessing Predictors of Software Defects\", Workshop on Predictive Software Models, Chicago", "segment": "**Author**: University of Massachusetts Vision Group, Carla Brodley  \n**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/image+segmentation) - 1990  \n**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)  \n\n**Image Segmentation Data Set**\nThe instances were drawn randomly from a database of 7 outdoor images. The images were hand-segmented to create a classification for every pixel. Each instance is a 3x3 region.\n \n__Major changes w.r.t. version 2: ignored first two variables as they do not fit the classification task (they reflect the location of the sample in the original image). The 3rd is constant, so should also be ignored.__\n\n\n### Attribute Information  \n\n4.  short-line-density-5:  the results of a line extractoin algorithm that \n          counts how many lines of length 5 (any orientation) with\n          low contrast, less than or equal to 5, go through the region.\n5.  short-line-density-2:  same as short-line-density-5 but counts lines\n          of high contrast, greater than 5.\n6.  vedge-mean:  measure the contrast of horizontally\n          adjacent pixels in the region.  There are 6, the mean and \n          standard deviation are given.  This attribute is used as\n         a vertical edge detector.\n7.  vegde-sd:  (see 6)\n8.  hedge-mean:  measures the contrast of vertically adjacent\n           pixels. Used for horizontal line detection. \n9.  hedge-sd: (see 8).\n10. intensity-mean:  the average over the region of (R + G + B)/3\n11. rawred-mean: the average over the region of the R value.\n12. rawblue-mean: the average over the region of the B value.\n13. rawgreen-mean: the average over the region of the G value.\n14. exred-mean: measure the excess red:  (2R - (G + B))\n15. exblue-mean: measure the excess blue:  (2B - (G + R))\n16. exgreen-mean: measure the excess green:  (2G - (R + B))\n17. value-mean:  3-d nonlinear transformation\n          of RGB. (Algorithm can be found in Foley and VanDam, Fundamentals\n          of Interactive Computer Graphics)\n18. saturatoin-mean:  (see 17)\n19. hue-mean:  (see 17)", "dna": "**Author**: Ross King, based on data from Genbank 64.1  \n\n**Source**: [MLbench](https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/DNA). Originally from the StatLog project.  \n\n**Please Cite**:   \n\n\n\n**Primate Splice-Junction Gene Sequences (DNA)**  \n\nOriginally from the StatLog project. The raw data is still available on [UCI](https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Splice-junction+Gene+Sequences)).\n\n\n\nThe data consists of 3,186 data points (splice junctions). The data points are described by 180 indicator binary variables and the problem is to recognize the 3 classes (ei, ie, neither), i.e., the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA sequence that are spliced out). The StatLog DNA dataset is a processed version of the [Irvine database]((https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Splice-junction+Gene+Sequences))). The main difference is that the symbolic variables representing the nucleotides (only A,G,T,C) were replaced by 3 binary indicator variables. Thus the original 60 symbolic attributes were changed into 180 binary attributes. The names of the examples were removed. The examples with ambiguities were removed (there was very few of them, 4). The StatLog version of this dataset was produced by Ross King at Strathclyde University. For original details see the Irvine database documentation.\n\n\n\nThe nucleotides A,C,G,T were given indicator values as follows:\n\nA -> 1 0 0\n\nC -> 0 1 0\n\nG -> 0 0 1\n\nT -> 0 0 0\n\n\n\nHint: Much better performance is generally observed if attributes closest to the junction are used. In the StatLog version, this means using attributes A61 to A120 only.", "kr-vs-kp": "Author: Alen Shapiro\nSource: [UCI](https://archive.ics.uci.edu/ml/datasets/Chess+(King-Rook+vs.+King-Pawn))\nPlease cite: [UCI citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)\n\n1. Title: Chess End-Game -- King+Rook versus King+Pawn on a7\n(usually abbreviated KRKPA7). The pawn on a7 means it is one square\naway from queening. It is the King+Rook's side (white) to move.\n\n2. Sources:\n(a) Database originally generated and described by Alen Shapiro.\n(b) Donor/Coder: Rob Holte (holte@uottawa.bitnet). The database\nwas supplied to Holte by Peter Clark of the Turing Institute\nin Glasgow (pete@turing.ac.uk).\n(c) Date: 1 August 1989\n\n3. Past Usage:\n- Alen D. Shapiro (1983,1987), \"Structured Induction in Expert Systems\",\nAddison-Wesley. This book is based on Shapiro's Ph.D. thesis (1983)\nat the University of Edinburgh entitled \"The Role of Structured\nInduction in Expert Systems\".\n- Stephen Muggleton (1987), \"Structuring Knowledge by Asking Questions\",\npp.218-229 in \"Progress in Machine Learning\", edited by I. Bratko\nand Nada Lavrac, Sigma Press, Wilmslow, England SK9 5BB.\n- Robert C. Holte, Liane Acker, and Bruce W. Porter (1989),\n\"Concept Learning and the Problem of Small Disjuncts\",\nProceedings of IJCAI. Also available as technical report AI89-106,\nComputer Sciences Department, University of Texas at Austin,\nAustin, Texas 78712.\n\n4. Relevant Information:\nThe dataset format is described below. Note: the format of this\ndatabase was modified on 2/26/90 to conform with the format of all\nthe other databases in the UCI repository of machine learning databases.\n\n5. Number of Instances: 3196 total\n\n6. Number of Attributes: 36\n\n7. Attribute Summaries:\nClasses (2): -- White-can-win (\"won\") and White-cannot-win (\"nowin\").\nI believe that White is deemed to be unable to win if the Black pawn\ncan safely advance.\nAttributes: see Shapiro's book.\n\n8. Missing Attributes: -- none\n\n9. Class Distribution:\nIn 1669 of the positions (52%), White can win.\nIn 1527 of the positions (48%), White cannot win.\n\nThe format for instances in this database is a sequence of 37 attribute values.\nEach instance is a board-descriptions for this chess endgame. The first\n36 attributes describe the board. The last (37th) attribute is the\nclassification: \"win\" or \"nowin\". There are 0 missing values.\nA typical board-description is\n\nf,f,f,f,f,f,f,f,f,f,f,f,l,f,n,f,f,t,f,f,f,f,f,f,f,t,f,f,f,f,f,f,f,t,t,n,won\n\nThe names of the features do not appear in the board-descriptions.\nInstead, each feature correponds to a particular position in the\nfeature-value list. For example, the head of this list is the value\nfor the feature \"bkblk\". The following is the list of features, in\nthe order in which their values appear in the feature-value list:\n\n[bkblk,bknwy,bkon8,bkona,bkspr,bkxbq,bkxcr,bkxwp,blxwp,bxqsq,cntxt,dsopp,dwipd,\nhdchk,katri,mulch,qxmsq,r2ar8,reskd,reskr,rimmx,rkxwp,rxmsq,simpl,skach,skewr,\nskrxp,spcop,stlmt,thrsk,wkcti,wkna8,wknck,wkovl,wkpos,wtoeg]\n\nIn the file, there is one instance (board position) per line.\n\n\nNum Instances: 3196\nNum Attributes: 37\nNum Continuous: 0 (Int 0 / Real 0)\nNum Discrete: 37\nMissing values: 0 / 0.0%", "Internet-Advertisements": "**Author**: Nicholas Kushmerick  \n\n**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements) - 1998  \n\n**Please cite**:   \n\n\n\n### Description\n\n\n\n__Changes to version 1:__ all categorical features transformed as such. \n\n\n\nThis dataset represents a set of possible advertisements on Internet pages. \n\n\n\n### Sources\n\n\n\n(a) Creator and donor:\n\n\n\nNicholas Kushmerick - nick@ucd.ie\n\n\n\n### Dataset Information\n\n\n\nThe features encode the geometry of the image (if available) as well as phrases occurring in the URL, the image's URL and alt text, the anchor text, and words occurring near the anchor text. The task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\").\n\n\n\n### Atributtes Information\n\n\n\nThere are : 3 continuous attributes. The others are binary.\n\nThis is the \"STANDARD encoding\" mentioned in the [Kushmerick, 99] (see below). \n\nOne or more of the three continuous features are missing in 28% of the instances.\n\nMissing values should be interpreted as \"unknown\".\n\n\n\n### Relevant Papers  \n\n\n\nN. Kushmerick (1999). \"Learning to remove Internet advertisements\", 3rd Int Conf Autonomous Agents.  \n\nAvailable at: http://rexa.info/paper/2fdc1cee89b7f4f2c9227d6f5d9b05d22c5ab3e9", "Bioresponse": "**Author**: Boehringer Ingelheim  \n**Source**: [Kaggle](https://www.kaggle.com/c/bioresponse) - 2011  \n**Please cite**: None  \n\nPredict a biological response of molecules from their chemical properties. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized.\n\nThe original training and test set were merged.", "churn": "**Author**: Unknown  \n\n**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/classification), [BigML](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383), Supposedly from UCI but I can't find it there.  \n\n**Please cite**:   \n\n\n\nA dataset relating characteristics of telephony account features and usage and whether or not the customer churned. Originally used in [Discovering Knowledge in Data: An Introduction to Data Mining](http://secs.ac.in/wp-content/CSE_PORTAL/DataMining_Daniel.pdf).", "first-order-theorem-proving": "**Author**: James P Bridge, Sean B Holden and Lawrence C Paulson \n\n  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/First-order+theorem+proving)  \n\n\n\n**Please cite**: James P Bridge, Sean B Holden and Lawrence C Paulson . Machine learning for first-order theorem proving: learning to select a good heuristic. Journal of Automated Reasoning, Springer 2012/13. \n\n\n\nSource:\n\n\n\nJames P Bridge, Sean B Holden and Lawrence C Paulson \n\n\n\nUniversity of Cambridge \n\nComputer Laboratory \n\nWilliam Gates Building \n\n15 JJ Thomson Avenue \n\nCambridge CB3 0FD \n\nUK \n\n\n\n+44 (0)1223 763500 \n\nforename.surname '@' cl.cam.ac.uk\n\n\n\n\n\nData Set Information:\n\n\n\nSee the file dataset file.\n\n\n\n\n\nAttribute Information:\n\n\n\nThe attributes are a mixture of static and dynamic features derived from theorems to be proved. See the paper for full details.", "GesturePhaseSegmentationProcessed": "**Author**: Renata Cristina Barros Madeo (Madeo\",\"R. C. B.)  Priscilla Koch Wagner (Wagner\",\"P. K.)  Sarajane Marques Peres (Peres\",\"S. M.)  {renata.si\",\"priscilla.wagner\",\"sarajane} at usp.br  http://each.uspnet.usp.br/sarajane/  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/gesture+phase+segmentation)  \n\n**Please cite**: Please refer to the [Machine Learning Repository's citation policy](https://archive.ics.uci.edu/ml/citation_policy.html). Additionally, the authors require a citation to one or more publications from those cited as relevant papers.  \n\n\n\nCreators: \n\nRenata Cristina Barros Madeo (Madeo, R. C. B.) \n\nPriscilla Koch Wagner (Wagner, P. K.) \n\nSarajane Marques Peres (Peres, S. M.) \n\n{renata.si, priscilla.wagner, sarajane} at usp.br \n\nhttp://each.uspnet.usp.br/sarajane/ \n\n\n\nDonor: \n\nUniversity of Sao Paulo - Brazil\n\n\n\nData Set Information:\n\n\n\nThe dataset is composed by features extracted from 7 videos with people gesticulating, aiming at studying Gesture Phase Segmentation. \n\nEach video is represented by two files: a raw file, which contains the position of hands, wrists, head and spine of the user in each frame; and a processed file, which contains velocity and acceleration of hands and wrists. See the data set description for more information on the dataset.\n\n\n\nAttribute Information:\n\n\n\nRaw files: 18 numeric attributes (double), a timestamp and a class attribute (nominal). \n\nProcessed files: 32 numeric attributes (double) and a class attribute (nominal). \n\nA feature vector with up to 50 numeric attributes can be generated with the two files mentioned above.\n\n\n\nThis is the processed data set with the following feature description:\n\n\n\n   Processed files:\n\n\n\n   1. Vectorial velocity of left hand (x coordinate)\n\n   2. Vectorial velocity of left hand (y coordinate)\n\n   3. Vectorial velocity of left hand (z coordinate)\n\n   4. Vectorial velocity of right hand (x coordinate)\n\n   5. Vectorial velocity of right hand (y coordinate)\n\n   6. Vectorial velocity of right hand (z coordinate)\n\n   7. Vectorial velocity of left wrist (x coordinate)\n\n   8. Vectorial velocity of left wrist (y coordinate)\n\n   9. Vectorial velocity of left wrist (z coordinate)\n\n   10. Vectorial velocity of right wrist (x coordinate)\n\n   11. Vectorial velocity of right wrist (y coordinate)\n\n   12. Vectorial velocity of right wrist (z coordinate)\n\n   13. Vectorial acceleration of left hand (x coordinate)\n\n   14. Vectorial acceleration of left hand (y coordinate)\n\n   15. Vectorial acceleration of left hand (z coordinate)\n\n   16. Vectorial acceleration of right hand (x coordinate)\n\n   17. Vectorial acceleration of right hand (y coordinate)\n\n   18. Vectorial acceleration of right hand (z coordinate)\n\n   19. Vectorial acceleration of left wrist (x coordinate)\n\n   20. Vectorial acceleration of left wrist (y coordinate)\n\n   21. Vectorial acceleration of left wrist (z coordinate)\n\n   22. Vectorial acceleration of right wrist (x coordinate)\n\n   23. Vectorial acceleration of right wrist (y coordinate)\n\n   24. Vectorial acceleration of right wrist (z coordinate)\n\n   25. Scalar velocity of left hand\n\n   26. Scalar velocity of right hand\n\n   27. Scalar velocity of left wrist\n\n   28. Scalar velocity of right wrist\n\n   29. Scalar velocity of left hand\n\n   30. Scalar velocity of right hand\n\n   31. Scalar velocity of left wrist\n\n   32. Scalar velocity of right wrist\n\n   33. phase:\n\n       - D (rest position, from portuguese \"descanso\")\n\n       - P (preparation)\n\n       - S (stroke)\n\n       - H (hold)\n\n       - R (retraction)\n\n\n\nRelevant Papers:\n\n\n\n1. Madeo, R. C. B. ; Lima, C. A. M. ; PERES, S. M. . Gesture Unit Segmentation using Support Vector Machines: Segmenting \n\nGestures from Rest Positions. In: Symposium on Applied Computing (SAC), 2013, Coimbra. Proceedings of the 28th Annual \n\nACM Symposium on Applied Computing (SAC), 2013. p. 46-52. \n\n* In this paper, the videos A1 and A2 were studied. \n\n\n\n2. Wagner, P. K. ; PERES, S. M. ; Madeo, R. C. B. ; Lima, C. A. M. ; Freitas, F. A. . Gesture Unit Segmentation Using \n\nSpatial-Temporal Information and Machine Learning. In: 27th Florida Artificial Intelligence Research Society Conference \n\n(FLAIRS), 2014, Pensacola Beach. Proceedings of the 27th Florida Artificial Intelligence Research Society Conference \n\n(FLAIRS). Palo Alto : The AAAI Press, 2014. p. 101-106. \n\n* In this paper, the videos A1, A2, A3, B1, B3, C1 and C3 were studied. \n\n\n\n3. Madeo, R. C. B.. Support Vector Machines and Gesture Analysis: incorporating temporal aspects (in Portuguese). Master \n\nThesis - Universidade de Sao Paulo, Sao Paulo Researcher Foundation. 2013. \n\n* In this document, the videos named B1 and B3 in the document correspond to videos C1 and C3 in this dataset. Only \n\nfive videos were explored in this document: A1, A2, A3, C1 and C3. \n\n\n\n4. Wagner, P. K. ; Madeo, R. C. B. ; PERES, S. M. ; Lima, C. A. M. . Segmenta&Atilde;&sect;ao de Unidades Gestuais com Multilayer \n\nPerceptrons (in Portuguese). In: Encontro Nacional de Inteligencia Artificial e Computacional (ENIAC), 2013, Fortaleza. \n\nAnais do X Encontro Nacional de Inteligencia Artificial e Computacional (ENIAC), 2013. \n\n* In this paper, the videos A1, A2 and A3 were studied.\n\n\n\n\n\n\n\nCitation Request:\n\n\n\nPlease refer to the Machine Learning Repository's citation policy. \n\nAdditionally, the authors require a citation to one or more publications from those cited as relevant papers.", "PhishingWebsites": "**Author**: Rami Mustafa A Mohammad ( University of Huddersfield\",\"rami.mohammad '@' hud.ac.uk\",\"rami.mustafa.a '@' gmail.com) Lee McCluskey (University of Huddersfield\",\"t.l.mccluskey '@' hud.ac.uk )  Fadi Thabtah (Canadian University of Dubai\",\"fadi '@' cud.ac.ae)  \n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/phishing+websites)  \n\n**Please cite**: Please refer to the [Machine Learning Repository's citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)  \n\n\n\nSource:\n\n\n\nRami Mustafa A Mohammad ( University of Huddersfield, rami.mohammad '@' hud.ac.uk, rami.mustafa.a '@' gmail.com)\n\nLee McCluskey (University of Huddersfield,t.l.mccluskey '@' hud.ac.uk )\n\nFadi Thabtah (Canadian University of Dubai,fadi '@' cud.ac.ae)\n\n\n\n\n\nData Set Information:\n\n\n\nOne of the challenges faced by our research was the unavailability of reliable training datasets. In fact this challenge faces any researcher in the field. However, although plenty of articles about predicting phishing websites have been disseminated these days, no reliable training dataset has been published publically, may be because there is no agreement in literature on the definitive features that characterize phishing webpages, hence it is difficult to shape a dataset that covers all possible features. \n\nIn this dataset, we shed light on the important features that have proved to be sound and effective in predicting phishing websites. In addition, we propose some new features.\n\n\n\n\n\nAttribute Information:\n\n\n\nFor Further information about the features see the features file in the [data folder](https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Phishing Websites Features.docx) of UCI.\n\n\n\nRelevant Papers:\n\n\n\nMohammad, Rami, McCluskey, T.L. and Thabtah, Fadi (2012) An Assessment of Features Related to Phishing Websites using an Automated Technique. In: International Conferece For Internet Technology And Secured Transactions. ICITST 2012 . IEEE, London, UK, pp. 492-497. ISBN 978-1-4673-5325-0\n\n\n\nMohammad, Rami, Thabtah, Fadi Abdeljaber and McCluskey, T.L. (2014) Predicting phishing websites based on self-structuring neural network. Neural Computing and Applications, 25 (2). pp. 443-458. ISSN 0941-0643\n\n\n\nMohammad, Rami, McCluskey, T.L. and Thabtah, Fadi Abdeljaber (2014) Intelligent Rule based Phishing Websites Classification. IET Information Security, 8 (3). pp. 153-160. ISSN 1751-8709\n\n\n\n \n\n\n\nCitation Request:\n\n\n\nPlease refer to the Machine Learning Repository's citation policy", "sylvine": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "christine": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "wine-quality-white": "Citation Request:\n  This dataset is public available for research. The details are described in [Cortez et al., 2009]. \n  Please include this citation if you plan to use this database:\n\n  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n  Modeling wine preferences by data mining from physicochemical properties.\n  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n\n  Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016\n                [Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf\n                [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib\n\n1. Title: Wine Quality \n\n2. Sources\n   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n   \n3. Past Usage:\n\n  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n  Modeling wine preferences by data mining from physicochemical properties.\n  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n\n  In the above reference, two datasets were created, using red and white wine samples.\n  The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n  between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n  these datasets under a regression approach. The support vector machine model achieved the\n  best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n  etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n  analysis procedure).\n \n4. Relevant Information:\n\n   The two datasets are related to red and white variants of the Portuguese &quot;Vinho Verde&quot; wine.\n   For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].\n   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\n   These datasets can be viewed as classification or regression tasks.\n   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n   or poor wines. Also, we are not sure if all input variables are relevant. So\n   it could be interesting to test feature selection methods. \n\n5. Number of Instances: red wine - 1599; white wine - 4898. \n\n6. Number of Attributes: 11 + output attribute\n  \n   Note: several of the attributes may be correlated, thus it makes sense to apply some sort of\n   feature selection.\n\n7. Attribute information:\n\n   For more information, read [Cortez et al., 2009].\n\n   Input variables (based on physicochemical tests):\n   1 - fixed acidity\n   2 - volatile acidity\n   3 - citric acid\n   4 - residual sugar\n   5 - chlorides\n   6 - free sulfur dioxide\n   7 - total sulfur dioxide\n   8 - density\n   9 - pH\n   10 - sulphates\n   11 - alcohol\n   Output variable (based on sensory data): \n   12 - quality (score between 0 and 10)\n\n8. Missing Attribute Values: None", "Satellite": "**Author**: Markus Goldstein  \n\n**Source**: [Dataverse](http://www.madm.eu/downloads https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF)  \n\n**Please cite**:   \n\n\n\nThe satellite dataset comprises of features extracted from satellite observations. In particular, each image was taken under four different light wavelength, two in visible light (green and red) and two infrared images. The task of the original dataset is to classify the image into the soil category of the observed region. \n\n\n\n### Classes\n\nWe defined the soil classes &ldquo;red soil&rdquo;, &ldquo;gray soil&rdquo;, &ldquo;damp gray soil&rdquo; and &ldquo;very damp gray soil&rdquo; as the normal class. From the semantically different classes &ldquo;cotton crop&rdquo; and &ldquo;soil with vegetation stubble&rdquo; anomalies are sampled. \n\n\n\nAfter merging the original training and test set into a single dataset, the resulting dataset contains 5,025 normal instances as well as 75 randomly sampled anomalies (1.49%) with 36 dimensions \n\n\n\n### Relevant Papers\n\n\n\nGoldstein, Markus, and Seiichi Uchida. A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data.&quot; PloS one 11.4 (2016): e0152173 \n\n\n\nThis dataset is not the original dataset. The target variable 'Target' is relabeled into 'Normal' and 'Anomaly'", "Fashion-MNIST": "**Author**: Han Xiao, Kashif Rasul, Roland Vollgraf  \n\n**Source**: [Zalando Research](https://github.com/zalandoresearch/fashion-mnist)  \n\n**Please cite**: Han Xiao and Kashif Rasul and Roland Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, arXiv, cs.LG/1708.07747  \n\n\n\nFashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. \n\n\n\nRaw data available at: https://github.com/zalandoresearch/fashion-mnist\n\n\n\n### Target classes\n\nEach training and test example is assigned to one of the following labels:\n\nLabel  Description  \n\n0  T-shirt/top  \n\n1  Trouser  \n\n2  Pullover  \n\n3  Dress  \n\n4  Coat  \n\n5  Sandal  \n\n6  Shirt  \n\n7  Sneaker  \n\n8  Bag  \n\n9  Ankle boot", "connect-4": "**Author**: John Tromp  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Connect-4) - 1995  \n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)  \n\n**Connect-4**  \nThis database contains all legal 8-ply positions in the game of connect-4 in which neither player has won yet, and in which the next move is not forced. Attributes represent board positions on a 6x6 board. The outcome class is the game-theoretical value for the first player (2: win, 1: loss, 0: draw).\n\n### Attribute Information  \n\nThe board is numbered like:  \n6 . . . . . . .  \n5 . . . . . . .  \n4 . . . . . . .  \n3 . . . . . . .  \n2 . . . . . . .  \n1 . . . . . . .  \na b c d e f g  \n\nThe values represent:  \n0: Blank  \n1: Taken by Player 1  \n2: Taken by Player 2", "Amazon_employee_access": "**Author**:   \n\n**Source**: [Kaggle Amazon Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)  \n\n**Please cite**:   \n\n\n\n### Description\n\n\n\nThe data consists of real historical data collected from 2010 & 2011.  Employees are manually allowed or denied access to resources over time. \n\nThe data is used to create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees.\n\n\n\n### Dataset Information\n\n\n\nWhen an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a non-trivial amount of time and money.\n\n\n\nThere is a considerable amount of data regarding an employee&rsquo;s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access.\n\n\n\nThe original training and test set were merged.\n\n\n\n### Attributes Information\n\n\n\n* ACTION [target]: ACTION is 1 if the resource was approved, 0 if the resource was not  \n\n* RESOURCE: An ID for each resource  \n\n* MGR_ID: The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time  \n\n* ROLE_ROLLUP_1: Company role grouping category id 1 (e.g. US Engineering)  \n\n* ROLE_ROLLUP_2: Company role grouping category id 2 (e.g. US Retail)  \n\n* ROLE_DEPTNAME: Company role department description (e.g. Retail)  \n\n* ROLE_TITLE: Company role business title description (e.g. Senior Engineering Retail Manager)  \n\n* ROLE_FAMILY_DESC: Company role family extended description (e.g. Retail Manager, Software Engineering)  \n\n* ROLE_FAMILY: Company role family description (e.g. Retail Manager)  \n\n* ROLE_CODE: Company role code; this code is unique to each role (e.g. Manager)", "nomao": "**Author**: Nomao Labs\n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Nomao)\n\n**Please cite**: Laurent Candillier and Vincent Lemaire. Design and Analysis of the Nomao Challenge - Active Learning in the Real-World. In: Proceedings of the ALRA : Active Learning in Real-world Applications, Workshop ECML-PKDD 2012, Friday, September 28, 2012, Bristol, UK.\n\n1. Data set title:\nNomao Data Set \n\n\n2. Abstract: \nNomao collects data about places (name, phone, localization...) from many sources. Deduplication consists in detecting what data refer to the same place. Instances in the dataset compare 2 spots.\n\n3. Data Set Characteristics:  \n\n- Univariate\n- Area: Computer\n- Attribute Characteristics: Real\n- Associated Tasks: Classification\n- Missing Values?: Yes\n\n\n4. Source:\n\n(a) Original owner of database (name / phone / snail address / email address) \nNomao / 00 33 5 62 48 33 90 / 1 avenue Jean Rieux, 31500 Toulouse / challenge '@' nomao.com \n(b) Donor of database (name / phone / snail address / email address) \nLaurent Candillier / - / 1 avenue Jean Rieux, 31500 Toulouse / laurent '@' nomao.com\n\n\n5. Data Set Information:\n\nThe dataset has been enriched during the Nomao Challenge: organized along with the ALRA workshop (Active Learning in Real-world Applications): held at the ECML-PKDD 2012 conference.\n\n5.1. Number of Instances\n\n34,465 instances, mix of continuous and nominal, labeled by human expert.\n\nFirst 29,104 instances have been labeled with \"human prior\".\nSee the corresponding article described in section \"3. Past Usage\" for more details.\n\nNext 917 instances have been labeled using the active learning method called \"marg\".\nNext 964 instances refer to the active method called \"wmarg\".\nNext 995 instances refer to the active method called \"wmarg5\".\nNext 1,985 instances refer to the active method called \"rand\" (random selection).\n\nLast instances have been labeled during the corresponding challenge.\nMore details can be found in http://www.nomao.com/labs/challenge\nNext 163 instances refer to the active method called \"baseline\".\nNext 167 instances refer to the active method called \"nomao\".\nAnd last 170 instances refer to the active method called \"tsun\".\n\n5.2. Number of Attributes \n\n120 attributes: 89 continuous, 31 nominal (including the attributes 'label' and 'id'). \n\nThe features are separated by comma.\n\n5.3. Attribute Information: \n\nMissing data are allowed, represented by question marks '?'.\n\nLabels are +1 if the concerned spots must be merged, -1 if they do not refer to the same entity.\n\n1 id: name is composed of the names of the spots that are compared, separated by a sharp (#).   \n2 clean_name_intersect_min: continuous.   \n3 clean_name_intersect_max: continuous.   \n4 clean_name_levenshtein_sim: continuous.   \n5 clean_name_trigram_sim: continuous.   \n6 clean_name_levenshtein_term: continuous.   \n7 clean_name_trigram_term: continuous.   \n8 clean_name_including: n,s,m.   \n9 clean_name_equality: n,s,m.   \n10 city_intersect_min: continuous.   \n11 city_intersect_max: continuous.   \n12 city_levenshtein_sim: continuous.   \n13 city_trigram_sim: continuous.   \n14 city_levenshtein_term: continuous.   \n15 city_trigram_term: continuous.   \n16 city_including: n,s,m.   \n17 city_equality: n,s,m.   \n18 zip_intersect_min: continuous.   \n19 zip_intersect_max: continuous.   \n20 zip_levenshtein_sim: continuous.   \n21 zip_trigram_sim: continuous.   \n22 zip_levenshtein_term: continuous.   \n23 zip_trigram_term: continuous.   \n24 zip_including: n,s,m.   \n25 zip_equality: n,s,m.   \n26 street_intersect_min: continuous.   \n27 street_intersect_max: continuous.   \n28 street_levenshtein_sim: continuous.   \n29 street_trigram_sim: continuous.   \n30 street_levenshtein_term: continuous.   \n31 street_trigram_term: continuous.   \n32 street_including: n,s,m.   \n33 street_equality: n,s,m.   \n34 website_intersect_min: continuous.   \n35 website_intersect_max: continuous.   \n36 website_levenshtein_sim: continuous.   \n37 website_trigram_sim: continuous.   \n38 website_levenshtein_term: continuous.   \n39 website_trigram_term: continuous.   \n40 website_including: n,s,m.   \n41 website_equality: n,s,m.   \n42 countryname_intersect_min: continuous.   \n43 countryname_intersect_max: continuous.   \n44 countryname_levenshtein_sim: continuous.   \n45 countryname_trigram_sim: continuous.   \n46 countryname_levenshtein_term: continuous.   \n47 countryname_trigram_term: continuous.   \n48 countryname_including: n,s,m.   \n49 countryname_equality: n,s,m.   \n50 geocoderlocalityname_intersect_min: continuous.   \n51 geocoderlocalityname_intersect_max: continuous.   \n52 geocoderlocalityname_levenshtein_sim: continuous.   \n53 geocoderlocalityname_trigram_sim: continuous.   \n54 geocoderlocalityname_levenshtein_term: continuous.   \n55 geocoderlocalityname_trigram_term: continuous.   \n56 geocoderlocalityname_including: n,s,m.   \n57 geocoderlocalityname_equality: n,s,m.   \n58 geocoderinputaddress_intersect_min: continuous.   \n59 geocoderinputaddress_intersect_max: continuous.   \n60 geocoderinputaddress_levenshtein_sim: continuous.   \n61 geocoderinputaddress_trigram_sim: continuous.   \n62 geocoderinputaddress_levenshtein_term: continuous.   \n63 geocoderinputaddress_trigram_term: continuous.   \n64 geocoderinputaddress_including: n,s,m.   \n65 geocoderinputaddress_equality: n,s,m.   \n66 geocoderoutputaddress_intersect_min: continuous.   \n67 geocoderoutputaddress_intersect_max: continuous.   \n68 geocoderoutputaddress_levenshtein_sim: continuous.   \n69 geocoderoutputaddress_trigram_sim: continuous.   \n70 geocoderoutputaddress_levenshtein_term: continuous.   \n71 geocoderoutputaddress_trigram_term: continuous.   \n72 geocoderoutputaddress_including: n,s,m.   \n73 geocoderoutputaddress_equality: n,s,m.   \n74 geocoderpostalcodenumber_intersect_min: continuous.   \n75 geocoderpostalcodenumber_intersect_max: continuous.   \n76 geocoderpostalcodenumber_levenshtein_sim: continuous.   \n77 geocoderpostalcodenumber_trigram_sim: continuous.   \n78 geocoderpostalcodenumber_levenshtein_term: continuous.   \n79 geocoderpostalcodenumber_trigram_term: continuous.   \n80 geocoderpostalcodenumber_including: n,s,m.   \n81 geocoderpostalcodenumber_equality: n,s,m.   \n82 geocodercountrynamecode_intersect_min: continuous.   \n83 geocodercountrynamecode_intersect_max: continuous.   \n84 geocodercountrynamecode_levenshtein_sim: continuous.   \n85 geocodercountrynamecode_trigram_sim: continuous.   \n86 geocodercountrynamecode_levenshtein_term: continuous.   \n87 geocodercountrynamecode_trigram_term: continuous.   \n88 geocodercountrynamecode_including: n,s,m.   \n89 geocodercountrynamecode_equality: n,s,m.   \n90 phone_diff: continuous.   \n91 phone_levenshtein: continuous.   \n92 phone_trigram: continuous.   \n93 phone_equality: n,s,m.   \n94 fax_diff: continuous.   \n95 fax_levenshtein: continuous.   \n96 fax_trigram: continuous.   \n97 fax_equality: n,s,m.   \n98 street_number_diff: continuous.   \n99 street_number_levenshtein: continuous.   \n100 street_number_trigram: continuous.   \n101 street_number_equality: n,s,m.   \n102 geocode_coordinates_long_diff: continuous.   \n103 geocode_coordinates_long_levenshtein: continuous.   \n104 geocode_coordinates_long_trigram: continuous.   \n105 geocode_coordinates_long_equality: n,s,m.   \n106 geocode_coordinates_lat_diff: continuous.   \n107 geocode_coordinates_lat_levenshtein: continuous.   \n108 geocode_coordinates_lat_trigram: continuous.   \n109 geocode_coordinates_lat_equality: n,s,m.   \n110 coordinates_long_diff: continuous.   \n111 coordinates_long_levenshtein: continuous.   \n112 coordinates_long_trigram: continuous.   \n113 coordinates_long_equality: n,s,m.   \n114 coordinates_lat_diff: continuous.   \n115 coordinates_lat_levenshtein: continuous.   \n116 coordinates_lat_trigram: continuous.   \n117 coordinates_lat_equality: n,s,m.   \n118 geocode_coordinates_diff: continuous.   \n119 coordinates_diff: continuous.   \n120 label: +1,-1.\n\nRelevant Papers: Laurent Candillier and Vincent Lemaire. Design and Analysis of the Nomao Challenge - Active Learning in the Real-World. In: Proceedings of the ALRA : Active Learning in Real-world Applications, Workshop ECML-PKDD 2012, Friday, September 28, 2012, Bristol, UK.", "jungle_chess_2pcs_raw_endgame_complete": "### Description ###\n\n\n\nThis dataset is part of a collection datasets based on the game \"Jungle Chess\" (a.k.a. Dou Shou Qi). For a description of the rules, please refer to the paper (link attached). The paper also contains a description of various constructed features. As the tablebases are a disjoint set of several tablebases based on which (two) pieces are on the board, we have uploaded all tablebases that have explicit different content:\n\n\n\n* Rat vs Rat\n\n* Rat vs Panther\n\n* Rat vs. Lion\n\n* Rat vs. Elephant\n\n* Panther vs. Lion\n\n* Panther vs. Elephant\n\n* Tiger vs. Lion\n\n* Lion vs. Lion\n\n* Lion vs. Elephant\n\n* Elephant vs. Elephant\n\n* Complete (Combination of the above)\n\n* RAW Complete (Combination of the above, containing for both pieces just the rank, file and strength information). This dataset contains a similar classification problem as, e.g., the King and Rook vs. King problem and is suitable for classification tasks. \n\n\n\n(Note that this dataset is one of the above mentioned datasets). Additionally, note that several subproblems are very similar. Having seen a given positions from one of the tablebases arguably gives a lot of information about the outcome of the same position in the other tablebases. \n\n\n\n### Please cite ###\n\nJ. N. van Rijn and J. K. Vis, Endgame Analysis of Dou Shou Qi. ICGA Journal 37:2, 120--124, 2014. ArXiv link: https://arxiv.org/abs/1604.07312", "bank-marketing": "**Author**: Paulo Cortez, S\u00e9rgio Moro\n\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/bank+marketing)\n\n**Please cite**: S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimar\u00e3es, Portugal, October, 2011. EUROSIS.       \n\n\n\n**Bank Marketing**  \n\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. \n\n\n\nThe classification goal is to predict if the client will subscribe a term deposit (variable y).\n\n\n\n### Attribute information  \n\nFor more information, read [Moro et al., 2011].\n\n\n\nInput variables:\n\n\n\n- bank client data:\n\n\n\n1 - age (numeric) \n\n\n\n2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\", \"student\",\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \n\n\n\n3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\"  means divorced or widowed) \n\n\n\n4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\") \n\n\n\n5 - default: has credit in default? (binary: \"yes\",\"no\") \n\n\n\n6 - balance: average yearly balance, in euros (numeric) \n\n\n\n7 - housing: has housing loan? (binary: \"yes\",\"no\") \n\n\n\n8 - loan: has personal loan? (binary: \"yes\",\"no\")\n\n\n\n- related with the last contact of the current campaign:\n\n\n\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n\n\n\n10 - day: last contact day of the month (numeric)\n\n\n\n11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n\n\n\n12 - duration: last contact duration, in seconds (numeric)\n\n\n\n- other attributes:\n\n\n\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n\n\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted) \n\n\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric) \n\n\n\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n\n \n\n- output variable (desired target):\n\n\n\n17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")", "adult": "**Author**: Ronny Kohavi and Barry Becker  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Adult) - 1996  \n**Please cite**: Ron Kohavi, \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996  \n\nPrediction task is to determine whether a person makes over 50K a year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n\nThis is the original version from the UCI repository, with training and test sets merged.\n\n### Variable description\n\nVariables are all self-explanatory except __fnlwgt__. This is a proxy for the demographic background of the people: \"People with similar demographic characteristics should have similar weights\". This similarity-statement is not transferable across the 51 different states.\n\nDescription from the donor of the database: \n\nThe weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\n1.  A single cell estimate of the population 16+ for each state.\n2.  Controls for Hispanic Origin by age and sex.\n3.  Controls by Race, age and sex.\n\nWe use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.\n\n\n### Relevant papers  \n\nRonny Kohavi and Barry Becker. Data Mining and Visualization, Silicon Graphics.  \ne-mail: ronnyk '@' live.com for questions.", "helena": "**SOURCE:** [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data)\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\n**NOTE:** This dataset corresponds to one of the datasets of the challenge.", "volkert": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.", "robert": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "shuttle": "Source: [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle))\n\nDonor:\n\nJason Catlett\nBasser Department of Computer Science,\nUniversity of Sydney, N.S.W., Australia\n\n\n\nData Set Information:\n\nApproximately 80% of the data belongs to class 1. Therefore the default accuracy is about 80%. The aim here is to obtain an accuracy of 99 - 99.9%.\n\nThe examples in the original dataset were in time order, and this time order could presumably be relevant in classification. However, this was not deemed relevant for StatLog purposes, so the order of the examples in the original dataset was randomised, and a portion of the original dataset removed for validation purposes.\n\n\nAttribute Information:\n\nThe shuttle dataset contains 9 attributes all of which are numerical. The first one being time. The last column is the class which has been coded as follows :\n1 Rad Flow\n2 Fpv Close\n3 Fpv Open\n4 High\n5 Bypass\n6 Bpv Close\n7 Bpv Open\n\n\nRelevant Papers:\n\nN/A", "guillermo": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "riccardo": "The goal of this challenge is to expose the research community to real world datasets of interest to 4Paradigm. All datasets are formatted in a uniform way, though the type of data might differ. The data are provided as preprocessed matrices, so that participants can focus on classification, although participants are welcome to use additional feature extraction procedures (as long as they do not violate any rule of the challenge). All problems are binary classification problems and are assessed with the normalized Area Under the ROC Curve (AUC) metric (i.e. 2*AUC-1).\n                   The identity of the datasets and the type of data is concealed, though its structure is revealed. The final score in  phase 2 will be the average of rankings  on all testing datasets, a ranking will be generated from such results, and winners will be determined according to such ranking.\n                   The tasks are constrained by a time budget. The Codalab platform provides computational resources shared by all participants. Each code submission will be exceuted in a compute worker with the following characteristics: 2Cores / 8G Memory / 40G SSD with Ubuntu OS. To ensure the fairness of the evaluation, when a code submission is evaluated, its execution time is limited in time.\n                   http://automl.chalearn.org/data", "MiniBooNE": "**Source:** [UCI](https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification#)\n\nByron Roe (byronroe '@' umich.edu)\nDepartment of Physics University of Michigan\nAnn Arbor, MI 48109\n\n\n**Data Set Information:**\n\nThis dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background).\n\nThis dataset is ordered. It first contains all signal observations, and then background observations.\n\n\n**Attribute Information:**\n\n50 particle ID variables (real) for each event.\n\n\n**Relevant Papers:**\n\nB. Roe et al., 'Boosted Decision Trees, an Alternative to Artificial Neural Networks' <[Web Link](https://arxiv.org/abs/physics/0408124)>,\narXiv:physics/0408124, Nucl. Instrum. Meth. A543, 577 (2005).", "kick": "One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases &quot;kicks&quot;.\n\n\n\nKicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle.\n\n\n\nModelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers.\n\n\n\nThe challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).", "Click_prediction_small": "This is the same data as version 5 (OpenML ID = 1220) with '_id' features coded as nominal factor variables.", "okcupid-stem": "User profile data for San Francisco OkCupid users published in [Kim, A. Y., & Escobedo-Land, A. (2015). OKCupid data for introductory statistics and data science courses. Journal of Statistics Education, 23(2).]. The curated dataset was downloaded from [https://github.com/rudeboybert/JSE_OkCupid]. The original dataset was created with the use of a python script that pulled the data from public profiles on www.okcupid.com on 06/30/2012. It includes people (n = 59946) within a 25 mile radius of San Francisco, who were online in the last year (06/30/2011), with at least one profile picture. Permission to use this data was obtained by the author of the original paper from OkCupid president and co-founder Christian Rudder under the condition that the dataset remains public. As target, the variable 'job' was collapsed into three categories: 'stem', 'non_stem', and 'student'. STEM jobs were defined as 'job' %in% c('computer / hardware / software', 'science / tech / engineering'). Observations with 'job' %in% c('unemployed', 'retired', 'rather not say') or missing values in 'job' were removed. The original dataset also included ten open text variables 'essay0' to 'essay9', which were removed from the dataset uploaded here. The dataset further includes the date/time variable 'last_online' (ignored by default) which could be used to construct additional features. Using OkCupid data for predicting STEM jobs was inspired by Max Kuhns book 'Feature Engineering and Selection: A Practical Approach for Predictive Models' [https://bookdown.org/max/FES/].", "sf-police-incidents": "Incident reports from the San Franciso Police Department between January 2003 and May 2018, provided by the City and County of San Francisco. The dataset was downloaded on 05.11.2018. from [https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry]. For a description of all variables, checkout the homepage of the data provider. The original data was published under ODC Public Domain Dedication and Licence (PDDL) [https://opendatacommons.org/licenses/pddl/1.0/]. As target, the binary variable 'ViolentCrime' was created. A 'ViolentCrime' was defined as 'Category' %in% c('ASSAULT', 'ROBBERY', 'SEX OFFENSES, FORCIBLE', 'KIDNAPPING') | 'Descript' %in% c('GRAND THEFT PURSESNATCH', 'ATTEMPTED GRAND THEFT PURSESNATCH'). Additional date and time features 'Hour', 'DayOfWeek', 'Month', and 'Year' were created. The original variables 'Category', 'Descript', 'Date', 'Time', 'Resolution', 'Location', and 'PdId' were removed from the dataset. One record which contained the only missing value in the variable 'PdDistrict' was removed from the dataset. Using this dataset for machine learning was inspired by Nina Zumel's blogpost [http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/]. Note that incidents consist of multiple rows in the dataset when the crime belongs to more than one 'Category', which is indicated by the ID variable 'IncidntNum' (ignored by default).", "KDDCup99": "INTRUSION DETECTOR LEARNING\n\nSoftware to detect network intrusions protects a computer network from unauthorized users, including perhaps insiders.  The intrusion detector learning task is to build a predictive model (i.e. a classifier) capable of distinguishing between \"bad\" connections, called intrusions or attacks, and \"good\" normal connections.\nThe 1998 DARPA Intrusion Detection Evaluation Program was prepared and managed by MIT Lincoln Labs. The objective was to survey and evaluate research in intrusion detection.  A standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment, was provided.  The 1999 KDD intrusion detection contest uses a version of this dataset.\n\nLincoln Labs set up an environment to acquire nine weeks of raw TCP dump data for a local-area network (LAN) simulating a typical U.S. Air Force LAN.  They operated the LAN as if it were a true Air Force environment, but peppered it with multiple attacks.\n\nThe raw training data was about four gigabytes of compressed binary TCP dump data from seven weeks of network traffic.  This was processed into about five million connection records.  Similarly, the two weeks of test data yielded around two million connection records.\n\nA connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.  Each connection is labeled as either normal, or as an attack, with exactly one specific attack type.  Each connection record consists of about 100 bytes.\n\nAttacks fall into four main categories:\n\n* DOS: denial-of-service, e.g. syn flood;\n* R2L: unauthorized access from a remote machine, e.g. guessing password;\n* U2R:  unauthorized access to local superuser (root) privileges, e.g., various \"buffer overflow\" attacks;\n* probing: surveillance and other probing, e.g., port scanning.\nIt is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data.  This makes the task more realistic.  Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants.  The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only. \n  \n \nDERIVED FEATURES\n\nStolfo et al. defined higher-level features that help in distinguishing normal connections from attacks.  There are several categories of derived features.\nThe \"same host\" features examine only the connections in the past two seconds that have the same destination host as the current connection, and calculate statistics related to protocol behavior, service, etc.\n\nThe similar \"same service\" features examine only the connections in the past two seconds that have the same service as the current connection.\n\n\"Same host\" and \"same service\" features are together called  time-based traffic features of the connection records.\n\nSome probing attacks scan the hosts (or ports) using a much larger time interval than two seconds, for example once per minute.  Therefore, connection records were also sorted by destination host, and features were constructed using a window of 100 connections to the same host instead of a time window.  This yields a set of so-called host-based traffic features.\n\nUnlike most of the DOS and probing attacks, there appear to be no sequential patterns that are frequent in records of R2L and U2R attacks. This is because the DOS and probing attacks involve many connections to some host(s) in a very short period of time, but the R2L and U2R attacks are embedded in the data portions \nof packets, and normally involve only a single connection.\n\nUseful algorithms for mining the unstructured data portions of packets automatically are an open research question.  Stolfo et al. used domain knowledge to add features that look for suspicious behavior in the data portions, such as the number of failed login attempts.  These features are called \"content\" features.\n\nA complete listing of the set of features defined for the connection records is given in the three tables below.  The data schema of the contest dataset is available in machine-readable form . \n  \n \n\nfeature namedescription \ttype\n\n* duration \tlength (number of seconds) of the connection \tcontinuous\n* protocol_type \ttype of the protocol, e.g. tcp, udp, etc. \tdiscrete\n* service \tnetwork service on the destination, e.g., http, telnet, etc. \tdiscrete\n* src_bytes \tnumber of data bytes from source to destination \tcontinuous\n* dst_bytes \tnumber of data bytes from destination to source \tcontinuous\n* flag \tnormal or error status of the connection \tdiscrete \n* land \t1 if connection is from/to the same host/port; 0 otherwise \tdiscrete\n* wrong_fragment \tnumber of \"wrong\" fragments \tcontinuous\n* urgent \tnumber of urgent packets \tcontinuous\n  \nTable 1: Basic features of individual TCP connections.\n \nfeature namedescription \ttype\n\n* hot \tnumber of \"hot\" indicatorscontinuous\n* num_failed_logins \tnumber of failed login attempts \tcontinuous\n* logged_in \t1 if successfully logged in; 0 otherwise \tdiscrete\n* num_compromised \tnumber of \"compromised\" conditions \tcontinuous\n* root_shell \t1 if root shell is obtained; 0 otherwise \tdiscrete\n* su_attempted \t1 if \"su root\" command attempted; 0 otherwise \tdiscrete\n* num_root \tnumber of \"root\" accesses \tcontinuous\n* num_file_creations \tnumber of file creation operations \tcontinuous\n* num_shells \tnumber of shell prompts \tcontinuous\n* num_access_files \tnumber of operations on access control files \tcontinuous\n* num_outbound_cmdsnumber of outbound commands in an ftp session \tcontinuous\n* is_hot_login \t1 if the login belongs to the \"hot\" list; 0 otherwise \tdiscrete\n* is_guest_login \t1 if the login is a \"guest\"login; 0 otherwise \tdiscrete\n  \nTable 2: Content features within a connection suggested by domain knowledge.\n \nfeature namedescription \ttype\n\n* count \tnumber of connections to the same host as the current connection in the past two seconds \tcontinuous\nNote: The following  features refer to these same-host connections.\n* serror_rate \t% of connections that have \"SYN\" errors \tcontinuous\n* rerror_rate \t% of connections that have \"REJ\" errors \tcontinuous\n* same_srv_rate \t% of connections to the same service \tcontinuous\n* diff_srv_rate \t% of connections to different services \tcontinuous\n* srv_count \tnumber of connections to the same service as the current connection in the past two seconds \tcontinuous\nNote: The following features refer to these same-service connections.\n* srv_serror_rate \t% of connections that have \"SYN\" errors \tcontinuous\n* srv_rerror_rate \t% of connections that have \"REJ\" errors \tcontinuous\n* srv_diff_host_rate \t% of connections to different hosts \tcontinuous \n  \nTable 3: Traffic features computed using a two-second time window.", "porto-seguro": "Training dataset of the 'Porto Seguros Safe Driver Prediction' Kaggle challenge [https://www.kaggle.com/c/porto-seguro-safe-driver-prediction]. The goal was to predict whether a driver will file an insurance claim next year. The official rules of the challenge explicitely state that the data may be used for 'academic research and education, and other non-commercial purposes' [https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/rules]. For a description of all variables checkout the Kaggle dataset repository [https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data]. It states that numeric features with integer values that do not contain 'bin' or 'cat' in their variable names are in fact ordinal features which could be treated as ordinal factors in R. For further information on effective preprocessing and feature engineering checkout the 'Kernels' section of the Kaggle challenge website [https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/kernels]. Note that many Kagglers removed all 'calc' variables as they do not seem to carry much information.", "Higgs": "This is a smaller version of the original dataset, containing 1M rows. \n**Author**: Daniel Whiteson, University of California Irvine  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/HIGGS)  \n**Please cite**: Baldi, P., P. Sadowski, and D. Whiteson. Searching for Exotic Particles in High-energy Physics with Deep Learning. Nature Communications 5 (July 2, 2014).  \n\n**Higgs Boson detection data**. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. The last 500,000 examples are used as a test set.\n\n**Note: This is the UCI Higgs dataset, same as version 1, but it fixes the definition of the class attribute, which is categorical, not numeric.**\n\n\n### Attribute Information\n* The first column is the class label (1 for signal, 0 for background)\n* 21 low-level features (kinematic properties): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag\n* 7 high-level features derived by physicists: m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. \n\nFor more detailed information about each feature see the original paper.\n\nRelevant Papers:\n\nBaldi, P., P. Sadowski, and D. Whiteson. Searching for Exotic Particles in High-energy Physics with Deep Learning. Nature Communications 5 (July 2, 2014).", "KDDCup09-Upselling": "This is the full version of the KDD Cup 2009 dataset\n\n**This Year's Challenge**\n\nCustomer Relationship Management (CRM) is a key element of modern marketing strategies. The KDD Cup 2009 offers the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling).\n\nThe most practical way, in a CRM system, to build knowledge on customer is to produce scores. A score (the output of a model) is an evaluation for all instances of a target variable to explain (i.e. churn, appetency or up-selling). Tools which produce scores allow to project, on a given population, quantifiable information. The score is computed using input variables which describe instances. Scores are then used by the information system (IS), for example, to personalize the customer relationship. An industrial customer analysis platform able to build prediction models with a very large number of input variables has been developed by Orange Labs. This platform implements several processing methods for instances and variables selection, prediction and indexation based on an efficient model combined with variable selection regularization and model averaging method. The main characteristic of this platform is its ability to scale on very large datasets with hundreds of thousands of instances and thousands of variables. The rapid and robust detection of the variables that have most contributed to the output prediction can be a key factor in a marketing application.\n\nThe challenge is to beat the in-house system developed by Orange Labs. It is an opportunity to prove that you can deal with a very large database, including heterogeneous noisy data (numerical and categorical variables), and unbalanced class distributions. Time efficiency is often a crucial point. Therefore part of the competition will be time-constrained to test the ability of the participants to deliver solutions quickly.\n\n**Task Description**\n\nThe task is to estimate the churn, appetency and up-selling probability of customers, hence there are three target values to be predicted. The challenge is staged in phases to test the rapidity with which each team is able to produce results. A large number of variables (15,000) is made available for prediction. However, to engage participants having access to less computing power, a smaller version of the dataset with only 230 variables will be made available in the second part of the challenge.\n\n* Churn (wikipedia definition): Churn rate is also sometimes called attrition rate. It is one of two primary factors that determine the steady-state level of customers a business will support. In its broadest sense, churn rate is a measure of the number of individuals or items moving into or out of a collection over a specific period of time. The term is used in many contexts, but is most widely applied in business with respect to a contractual customer base. For instance, it is an important factor for any business with a subscriber-based service model, including mobile telephone networks and pay TV operators. The term is also used to refer to participant turnover in peer-to-peer networks.\n\n* Appetency: In our context, the appetency is the propensity to buy a service or a product.\n\n* Up-selling (wikipedia definition): Up-selling is a sales technique whereby a salesman attempts to have the customer purchase more expensive items, upgrades, or other add-ons in an attempt to make a more profitable sale. Up-selling usually involves marketing more profitable services or products, but up-selling can also be simply exposing the customer to other options he or she may not have considered previously. Up-selling can imply selling something additional, or selling something that is more profitable or otherwise preferable for the seller instead of the original sale.\n\nThe training set contains 50,000 examples. The first predictive 14,740 variables are numerical and the last 260 predictive variables are categorical. The last target variable is binary (-1,1)."}